# import config
from functions.config import *

# import libraries
import os
import requests
from bs4 import BeautifulSoup
from tqdm.notebook import tqdm
import time

def get_courses_links():
    '''
    This function go get from the target website of this homework the link to the individual courses.
    To avoid useless web crawls we do not open the webpage if the file already exist.
    The number of courses depends on the number of pages we want to crawl and the number of coruses per page, both of which are defined as global parameters in functions/config.py.
    The crawled links are then inserted in a txt file which path is also defined as a global variable.
    '''

    # create main data folder if doesn't already exist
    if not os.path.exists(data_folder_path): 
        os.makedirs(data_folder_path)

    sleep_time = 2 # idle time between to requests, to avoid being blocked
    to_crawl = False # we check if the file already exists and has the right length, in this case we do not repeat the crawling

    # to avoid the site block the crawler considering it a bot we use a user agent taken from a real chrome session
    headers = {"user-agent": r"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36"}

    if not os.path.exists(courses_urls_path): # if file does not exist we have to crawl
        print("File does not exist! Crawling...")
        to_crawl = True

    if to_crawl == False: # if file is incomplete we have to crawl
        with open(courses_urls_path, 'r') as file:
            file_length = len(file.readlines())
            if file_length < n_courses:
                print("File exist but is incomplete! Crawling...")
                to_crawl = True
            else:
                print("File already exist and is complete. Using the previous version.")


    # if data is missing go crawl
    if to_crawl == True:

        with open(courses_urls_path, 'w') as file: # open file, if already exist creates a new one
            for i in tqdm(range(n_pages)): # cycle trough every page
                url = r"https://www.findamasters.com/masters-degrees/msc-degrees/?PG=" + str(1 + i) # we compose the url

                # get the webpage
                webpage = requests.get(url, headers = headers)
                soup = BeautifulSoup(webpage.text)
                soup.prettify()

                tags = soup.find_all('a', {"class": "courseLink"})  # get the tags we are interested in

                if not tags:
                    raise IOError("Crawler has been blocked by the website. Try again with higher idle time.")

                for tag in tags: # for every tag get the course link and append to file
                    link = tag["href"]
                    file.write(r"https://www.findamasters.com" + link + "\n")

                time.sleep(sleep_time) # wait to avoid getting blocked

        # the file automatically close itself when the "with" section ends, saving the written lines

def crawl_pages():
    '''
    This function open the single courses pages and download relative .html file for later use.
    To avoid useless web crawls we do not open a webpage if the relative file already exist.
    The function try to crawl all the links listed in the .txt file generated by the previous function
    '''

    sleep_time = 2 # idle time between to requests, to avoid being blocked
    to_crawl = False
    headers = {"user-agent": r"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36"}

    # create folder if not exist already
    if not os.path.exists(courses_pages_path):
        os.makedirs(courses_pages_path)

    # we check if the files already exists, in this case we do not repeat the crawling
    files_count = 0
    for _, _, files in os.walk(courses_pages_path):
        files_count += len(files)

    if files_count < n_courses:
        print("Crawling...")
        to_crawl = True
    else:
        print("All files already crawled. Using the existing version.")

    # if data is missing go crawl
    if to_crawl == True:

        # make a folder for every page if not already created
        for i in range(1, n_pages + 1):
            folder_path = courses_pages_path + "page_" + str(i)
            if not os.path.exists(folder_path):
                os.makedirs(folder_path)

        # populate folders
        with open(courses_urls_path, 'r') as file_1:
            for i, course_url in tqdm(enumerate(file_1), total = n_courses):
                course_url = course_url.strip('\n')
                course_file_path = courses_pages_path + "page_" + str(1 + i // courses_per_page) + "/" + "course_" + str(1 + i % courses_per_page) + ".html"

                if not os.path.exists(course_file_path): # if already crawled do not repeat
                    # get page
                    webpage = requests.get(course_url, headers = headers)
                    soup = BeautifulSoup(webpage.text, "html.parser")

                    if soup.title.text == r"Just a moment...":
                        raise IOError("Crawler has been blocked by the website. Try again with higher idle time.")

                    # write file
                    with open(course_file_path, 'w+', encoding = "utf-8") as file_2:
                        # html_page = soup.prettify()
                        # file_2.write(str(html_page))
                        file_2.write(str(soup))
                    # the file automatically close itself when the "with" section ends, saving the written lines

                    time.sleep(sleep_time) # wait to avoid getting blocked

def check_html_files():
    '''
    This function check if we downloaded correctly all the html files.
    It divides the files in three groups: those which are downloaded incorrectly because the crawler was blocked,
    those which are downloaded incorrectly because not missing on the website,
    those which are downloaded correctly.
    '''

    print("Checking the correctness of the crawl operation...")

    blocked_pages = 0
    unaviable_pages = 0
    correct_pages = 0
    for root, _, files in tqdm(os.walk(courses_pages_path), total = n_pages + 1): # checks for files in 400 subfolders and on root folder, thus 401
        for file in files:
            course_file_path = os.path.join(root, file)

            with open(course_file_path, 'r', encoding = "utf-8") as html_file:
                html_content = html_file.read()

            soup = BeautifulSoup(html_content, "html.parser")
            page_title = soup.title.text

            if page_title == r"Just a moment...": # blocked during crawling
                blocked_pages += 1
                os.remove(course_file_path)
            elif page_title == r"FindAMasters | 500 Error : Internal Server Error": # missing on website
                unaviable_pages += 1
            else: # downloaded correctly
                correct_pages += 1

    print(blocked_pages, "pages were blocked during crawling and had been removed. If this value is not zero run the crawling again to get the missing pages.")
    print(unaviable_pages, "pages are not available on the website.")
    print(correct_pages, "pages have been correctly downloaded.")