{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notebook by:\n",
    "* Lorenzo Pannacci 1948926\n",
    "* Francesco Proietti 1873188\n",
    "* INSERT NAME SURNAME AND ID HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Startup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################\n",
    "# LIBRARIES DOWNLOAD #\n",
    "######################\n",
    "\n",
    "install_packages = False\n",
    "if install_packages:\n",
    "    %pip install beautifulsoup4 tqdm pandas numpy matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################\n",
    "# LIBRARIES IMPORT #\n",
    "####################\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from tqdm.notebook import tqdm\n",
    "import time\n",
    "import os\n",
    "import csv\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Data collection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this homework, there is no provided dataset. Instead, you have to build your own. Your search engine will run on text documents. So, here\n",
    "we detail the procedure to follow for the data collection. We strongly suggest you work on different modules when implementing the required functions. For example, you may have a ```crawler.py``` module, a ```parser.py``` module, and a ```engine.py``` module: this is a good practice that improves readability in reporting and efficiency in deploying the code. Be careful; you are likely dealing with exceptions and other possible issues! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1. Get the list of master's degree courses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start with the list of courses to include in your corpus of documents. In particular, we focus on web scrapping the [MSc Degrees](https://www.findamasters.com/masters-degrees/msc-degrees/). Next, we want you to **collect the URL** associated with each site in the list from the previously collected list.\n",
    "The list is long and split into many pages. Therefore, we ask you to retrieve only the URLs of the places listed in **the first 400 pages** (each page has 15 courses, so you will end up with 6000 unique master's degree URLs).\n",
    "\n",
    "The output of this step is a `.txt` file whose single line corresponds to the master's URL.\n",
    "\n",
    "---\n",
    "\n",
    "Firstly we observe that we can nagivate trough the different pages using the link `https://www.findamasters.com/masters-degrees/msc-degrees/?PG=n` and changing `n` with the number of the desidered page. We also observe that after 30 pages are loaded the site thinks we are a bot and don't let us in, to fix this we wait some seconds before we open a new page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_folder_path = r\"data/\"\n",
    "if not os.path.exists(data_folder_path):                    # create main data folder if doesn't already exist\n",
    "    os.makedirs(data_folder_path)\n",
    "\n",
    "courses_urls_path = data_folder_path + r\"courses_urls.txt\"  # file path of the txt file to create\n",
    "sleep_time = 2                                              # idle time between to requests, to avoid being blocke\n",
    "to_crawl = False                                            # we check if the file already exists and has the right length, in this case we do not repeat the crawling\n",
    "n_pages = 400                                               # number of pages to search trough\n",
    "n_courses = n_pages * 15                                    # total number of courses crawled\n",
    "\n",
    "# to avoid the site block the crawler considering it a bot we use a user agent taken from a real chrome session\n",
    "headers = {\"user-agent\": r\"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36\"}\n",
    "\n",
    "if not os.path.exists(courses_urls_path): # if file does not exist we have to crawl\n",
    "    print(\"File does not exist! Crawling...\")\n",
    "    to_crawl = True\n",
    "\n",
    "if to_crawl == False: # if file is incomplete we have to crawl\n",
    "    with open(courses_urls_path, 'r') as file:\n",
    "        file_length = len(file.readlines())\n",
    "        if file_length < n_courses:\n",
    "            print(\"File exist but is incomplete! Crawling...\")\n",
    "            to_crawl = True\n",
    "        else:\n",
    "            print(\"File already exist and is complete. Using the previous version.\")\n",
    "\n",
    "\n",
    "# if data is missing go crawl\n",
    "if to_crawl == True:\n",
    "\n",
    "    with open(\"data/courses_urls.txt\", 'w') as file: # open file, if already exist creates a new one\n",
    "        for i in tqdm(range(n_pages)): # cycle trough every page\n",
    "            url = r\"https://www.findamasters.com/masters-degrees/msc-degrees/?PG=\" + str(1 + i) # we compose the url\n",
    "\n",
    "            # get the webpage\n",
    "            webpage = requests.get(url, headers = headers)\n",
    "            soup = BeautifulSoup(webpage.text)\n",
    "            soup.prettify()\n",
    "\n",
    "            tags = soup.find_all('a', {\"class\": \"courseLink\"})  # get the tags we are interested in\n",
    "\n",
    "            if not tags:\n",
    "                raise IOError(\"Crawler has been blocked by the website. Try again with higher idle time.\")\n",
    "\n",
    "            for tag in tags: # for every tag get the course link and append to file\n",
    "                link = tag[\"href\"]\n",
    "                file.write(r\"https://www.findamasters.com\" + link + \"\\n\")\n",
    "\n",
    "            time.sleep(sleep_time) # wait to avoid getting blocked\n",
    "\n",
    "    # the file automatically close itself when the \"with\" section ends, saving the written lines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2. Crawl master's degree pages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you get all the URLs in the first 400 pages of the list, you:\n",
    "\n",
    "1. Download the HTML corresponding to each of the collected URLs.\n",
    "2. After you collect a single page, immediately save its `HTML` in a file. In this way, if your program stops for any reason, you will not lose the data collected up to the stopping point.\n",
    "3. Organize the downloaded `HTML` pages into folders. Each folder will contain the `HTML` of the courses on page 1, page 2, ... of the list of master's programs.\n",
    "   \n",
    "__Tip__: Due to the large number of pages you should download, you can use some methods that can help you shorten the time. If you employed a particular process or approach, kindly describe it.\n",
    "\n",
    "---\n",
    "\n",
    "As before we have to insert a idle time between the loading of two pages to avoid that the website block us. We can found whether we have been blocked by checking if the webpage ha the title \"`Just a moment...`\". This makes the operations particularly slow, to crawl all pages we have to wait a few hours. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "courses_pages_path = r\"data/courses_html_pages/\" # path of the folder containing all the subfolders with the html files\n",
    "\n",
    "# create folder if not exist already\n",
    "if not os.path.exists(courses_pages_path):\n",
    "    os.makedirs(courses_pages_path)\n",
    "\n",
    "# we check if the files already exists, in this case we do not repeat the crawling\n",
    "to_crawl = False\n",
    "files_count = 0\n",
    "for _, _, files in os.walk(courses_pages_path):\n",
    "    files_count += len(files)\n",
    "\n",
    "if files_count < n_courses:\n",
    "    print(\"Crawling...\")\n",
    "    to_crawl = True\n",
    "else:\n",
    "    print(\"All files already crawled. Using the existing version.\")\n",
    "\n",
    "# if data is missing go crawl\n",
    "if to_crawl == True:\n",
    "\n",
    "    # make a folder for every page if not already created\n",
    "    for i in range(1, 400 + 1):\n",
    "        folder_path = courses_pages_path + \"page_\" + str(i)\n",
    "        if not os.path.exists(folder_path):\n",
    "            os.makedirs(folder_path)\n",
    "\n",
    "    # populate folders\n",
    "    with open(courses_urls_path, 'r') as file_1:\n",
    "        for i, course_url in tqdm(enumerate(file_1), total = n_courses):\n",
    "            course_url = course_url.strip('\\n')\n",
    "            course_file_path = courses_pages_path + \"page_\" + str(1 + i // 15) + \"/\" + \"course_\" + str(1 + i % 15) + \".html\"\n",
    "\n",
    "            if not os.path.exists(course_file_path): # if already crawled do not repeat\n",
    "                # get page\n",
    "                webpage = requests.get(course_url, headers = headers)\n",
    "                soup = BeautifulSoup(webpage.text, \"html.parser\")\n",
    "\n",
    "                if soup.title.text == r\"Just a moment...\":\n",
    "                    raise IOError(\"Crawler has been blocked by the website. Try again with higher idle time.\")\n",
    "\n",
    "                # write file\n",
    "                with open(course_file_path, 'w+', encoding = \"utf-8\") as file_2:\n",
    "                    # html_page = soup.prettify()\n",
    "                    # file_2.write(str(html_page))\n",
    "                    file_2.write(str(soup))\n",
    "                # the file automatically close itself when the \"with\" section ends, saving the written lines\n",
    "\n",
    "                time.sleep(sleep_time) # wait to avoid getting blocked\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe that some pages as `https://www.findamasters.com/masters-degrees/course/emergency-management-and-resilience-msc/?i373d7361c25450` (page 215, course 3) are missing and gives us a filler webpage. We will have to treat those courses carefully as the only information we can get from those is the link. We can easily identify those kind of pages thanks to their title: \"`FindAMasters | 500 Error : Internal Server Error`\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# crawling correctness check\n",
    "\n",
    "print(\"Checking the correctness of the crawl operation...\")\n",
    "\n",
    "blocked_pages = 0\n",
    "unaviable_pages = 0\n",
    "correct_pages = 0\n",
    "for root, _, files in tqdm(os.walk(courses_pages_path), total = 401): # checks for files in 400 subfolders and on root folder, thus 401\n",
    "    for file in files:\n",
    "        course_file_path = os.path.join(root, file)\n",
    "\n",
    "        with open(course_file_path, 'r', encoding = \"utf-8\") as html_file:\n",
    "            html_content = html_file.read()\n",
    "\n",
    "        soup = BeautifulSoup(html_content, \"html.parser\")\n",
    "        page_title = soup.title.text\n",
    "\n",
    "        if page_title == r\"Just a moment...\": # blocked during crawling\n",
    "            blocked_pages += 1\n",
    "            os.remove(course_file_path)\n",
    "        elif page_title == r\"FindAMasters | 500 Error : Internal Server Error\": # missing on website\n",
    "            unaviable_pages += 1\n",
    "        else: # downloaded correctly\n",
    "            correct_pages += 1\n",
    "\n",
    "print(blocked_pages, \"pages were blocked during crawling and had been removed. If this value is not zero run the crawling again to get the missing pages.\")\n",
    "print(unaviable_pages, \"pages are not present on the website anymore.\")\n",
    "print(correct_pages, \"pages have been correctly downloaded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Parse downloaded pages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, you should have all the HTML documents about the master's degree of interest, and you can start to extract specific information. The list of the information we desire for each course and their format is as follows:\n",
    "\n",
    "1. Course Name (to save as ```courseName```): string;\n",
    "2. University (to save as ```universityName```): string;\n",
    "3. Faculty (to save as ```facultyName```): string\n",
    "4. Full or Part Time (to save as ```isItFullTime```): string;\n",
    "5. Short Description (to save as ```description```): string;\n",
    "6. Start Date (to save as ```startDate```): string;\n",
    "7. Fees (to save as ```fees```): string;\n",
    "8. Modality (to save as ```modality```):string;\n",
    "9. Duration (to save as ```duration```):string;\n",
    "10. City (to save as ```city```): string;\n",
    "11. Country (to save as ```country```): string;\n",
    "12. Presence or online modality (to save as ```administration```): string;\n",
    "13. Link to the page (to save as ```url```): string.\n",
    "\n",
    "<div style=\"overflow-x:auto;\">\n",
    "<table>\n",
    "<thead>\n",
    "  <tr>\n",
    "    <th>index</th>\n",
    "    <th>courseName</th>\n",
    "    <th>universityName</th>\n",
    "    <th>facultyName</th>\n",
    "    <th>isItFullTime</th>\n",
    "    <th>description</th>\n",
    "    <th>startDate</th>\n",
    "    <th>fees</th>\n",
    "    <th>modality</th>\n",
    "    <th>duration</th>\n",
    "    <th>city</th>\n",
    "    <th>country</th>\n",
    "    <th>administration</th>\n",
    "    <th>url</th>\n",
    "  </tr>\n",
    "</thead>\n",
    "<tbody>\n",
    "  <tr>\n",
    "    <td>0</td>\n",
    "    <td> Accounting and Finance - MSc</td>\n",
    "    <td>University of Leeds</td>\n",
    "    <td>Leeds University Business School</td>\n",
    "    <td>Full time</td>\n",
    "    <td>Businesses and governments rely on [...].</td>\n",
    "    <td>September</td>\n",
    "    <td>UK: £18,000 (Total) International: £34,750 (Total)</td>\n",
    "    <td>MSc</td>\n",
    "    <td>1 year full time</td>\n",
    "    <td>Leeds</td>\n",
    "    <td>United Kingdom</td>\n",
    "    <td>On Campus</td>\n",
    "    <td><a href=\"https://www.findamasters.com/masters-degrees/course/accounting-and-finance-msc/?i321d3232c3891\">Link</a></td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>1</td>\n",
    "    <td> Accounting, Accountability & Financial Management MSc</td>\n",
    "    <td>King’s College London</td>\n",
    "    <td>King’s Business School</td>\n",
    "    <td>Full time</td>\n",
    "    <td>Our Accounting, Accountability & Financial Management MSc course will provide [...].</td>\n",
    "    <td>September</td>\n",
    "    <td>Please see the university website for further information on fees for this course.</td>\n",
    "    <td>MSc</td>\n",
    "    <td>1 year FT</td>\n",
    "    <td>London</td>\n",
    "    <td>United Kingdom</td>\n",
    "    <td>On Campus</td>\n",
    "    <td><a href=\"https://www.findamasters.com/masters-degrees/course/accounting-accountability-and-financial-management-msc/?i132d7816c25522\">Link</a></td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>2</td>\n",
    "    <td> Accounting, Financial Management and Digital Business - MSc</td>\n",
    "    <td>University of Reading</td>\n",
    "    <td>Henley Business School</td>\n",
    "    <td>Full time</td>\n",
    "    <td>Embark on a professional accounting career [...].</td>\n",
    "    <td>September</td>\n",
    "    <td>Please see the university website for further information on fees for this course.</td>\n",
    "    <td>MSc</td>\n",
    "    <td>1 year full time</td>\n",
    "    <td>Reading</td>\n",
    "    <td>United Kingdom</td>\n",
    "    <td>On Campus</td>\n",
    "    <td><a href=\"https://www.findamasters.com/masters-degrees/course/accounting-financial-management-and-digital-business-msc/?i345d4286c351\">Link</a></td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>3</td>\n",
    "    <td> Addictions MSc</td>\n",
    "    <td>King’s College London</td>\n",
    "    <td>Institute of Psychiatry, Psychology and Neuroscience</td>\n",
    "    <td>Full time</td>\n",
    "    <td>Join us for an online session for prospective [...].</td>\n",
    "    <td>September</td>\n",
    "    <td>Please see the university website for further information on fees for this course.</td>\n",
    "    <td>MSc</td>\n",
    "    <td>One year FT</td>\n",
    "    <td>London</td>\n",
    "    <td>United Kingdom</td>\n",
    "    <td>On Campus</td>\n",
    "    <td><a href=\"https://www.findamasters.com/masters-degrees/course/addictions-msc/?i132d4318c27100\">Link</a></td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>4</td>\n",
    "    <td> Advanced Chemical Engineering - MSc</td>\n",
    "    <td>University of Leeds</td>\n",
    "    <td>School of Chemical and Process Engineering</td>\n",
    "    <td>Full time</td>\n",
    "    <td>The Advanced Chemical Engineering MSc at Leeds [...].</td>\n",
    "    <td>September</td>\n",
    "    <td>UK: £13,750 (Total) International: £31,000 (Total)</td>\n",
    "    <td>MSc</td>\n",
    "    <td>1 year full time</td>\n",
    "    <td>Leeds</td>\n",
    "    <td>United Kingdom</td>\n",
    "    <td>On Campus</td>\n",
    "  </tr>\n",
    "  <!-- Add more rows here as needed -->\n",
    "</tbody>\n",
    "</table>\n",
    "</div>\n",
    "\n",
    "\n",
    "For each master's degree, you create a `course_i.tsv` file of this structure:\n",
    "\n",
    "```\n",
    "courseName \\t universityName \\t  ... \\t url\n",
    "```\n",
    "\n",
    "If an information is missing, you just leave it as an empty string.\n",
    "\n",
    "---\n",
    "\n",
    "We can observe that some informations are \"mandatory\", that means that every page (that is not a filler page) has them while others could or could not be present. Meanwhile filler pages gives us no information whatsoever, for those we have only the page url. To create a `.tsv` file we can just use the Python `csv` module changing its delimiter with the character `\\t`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsvs_path = r\"data/tsvs/\" # path of the folder containing all the .tsv files\n",
    "\n",
    "# create folder if not exist already\n",
    "if not os.path.exists(tsvs_path):\n",
    "    os.makedirs(tsvs_path)\n",
    "\n",
    "# we check if all the files already exists, in this case we do not repeat the crawling\n",
    "to_crawl = False\n",
    "files_count = 0\n",
    "for _, _, files in os.walk(tsvs_path):\n",
    "    files_count += len(files)\n",
    "\n",
    "if files_count < n_courses:\n",
    "    print(\"Creating .tsv files...\")\n",
    "    to_crawl = True\n",
    "else:\n",
    "    print(\"All files already created. Using the existing version.\")\n",
    "\n",
    "# if data is missing go crawl\n",
    "if to_crawl == True:\n",
    "    with open(courses_urls_path, 'r') as courses_file:\n",
    "        for i, url in tqdm(enumerate(courses_file), total = n_courses):\n",
    "            url = url.strip(\"\\n\")\n",
    "\n",
    "            # if file .tsv already exist skip its creation\n",
    "            tsv_file_path = tsvs_path + \"course_\" + str(1 + i) + \".tsv\"\n",
    "            if os.path.exists(tsv_file_path):\n",
    "                continue\n",
    "\n",
    "            # create path, open and read .html file\n",
    "            course_file_path = courses_pages_path + \"page_\" + str(1 + i // 15) + \"/\" + \"course_\" + str(1 + i % 15) + \".html\"\n",
    "            with open(course_file_path, 'r', encoding = \"utf-8\") as html_file:\n",
    "                html_content = html_file.read()\n",
    "\n",
    "            soup = BeautifulSoup(html_content, \"html.parser\")\n",
    "\n",
    "            # if the page is no avaiable we can't get informations\n",
    "            if soup.title.text == r\"FindAMasters | 500 Error : Internal Server Error\":\n",
    "                courseName = universityName = facultyName = isItFullTime = description = startDate = fees = modality = duration = city = country = administration = \"\"\n",
    "\n",
    "            else:\n",
    "                # get all the required fields\n",
    "\n",
    "                courseName = soup.find(\"h1\", {\"class\": \"course-header__course-title\"}).get_text(strip = True)\n",
    "                universityName = soup.find(\"a\", {\"class\": \"course-header__institution\"}).get_text(strip = True)\n",
    "                facultyName = soup.find(\"a\", {\"class\": \"course-header__department\"}).get_text(strip = True)\n",
    "\n",
    "                # some entries do not have this field\n",
    "                extract = soup.find(\"span\", {\"class\": \"key-info__study-type\"})\n",
    "                if extract is None:\n",
    "                    isItFullTime = \"\"\n",
    "                else:\n",
    "                    isItFullTime = extract.get_text(strip = True)\n",
    "\n",
    "                description = soup.find(\"div\", {\"class\": \"course-sections__description\"}).find(\"div\", {\"class\": \"course-sections__content\"}).get_text(strip = True)\n",
    "                startDate = soup.find(\"span\", {\"class\": \"key-info__start-date\"}).get_text(strip = True)\n",
    "\n",
    "                # some entries do not have this field\n",
    "                extract = soup.find(\"div\", {\"class\": \"course-sections__fees\"})\n",
    "                if extract is None:\n",
    "                    fees = \"\"\n",
    "                else:\n",
    "                    fees = extract.find(\"div\", {\"class\": \"course-sections__content\"}).get_text(strip = True)\n",
    "\n",
    "                modality = soup.find(\"span\", {\"class\": \"key-info__qualification\"}).get_text(strip = True)\n",
    "                duration = soup.find(\"span\", {\"class\": \"key-info__duration\"}).get_text(strip = True)\n",
    "                city = soup.find(\"a\", {\"class\": \"course-data__city\"}).get_text(strip = True)\n",
    "                country = soup.find(\"a\", {\"class\": \"course-data__country\"}).get_text(strip = True)\n",
    "\n",
    "                # courses can be 'on_campus', 'online' or both, but this information is stored in different tags\n",
    "                extract1 = soup.find(\"a\", {\"class\": \"course-data__online\"})\n",
    "                extract2 = soup.find(\"a\", {\"class\": \"course-data__on-campus\"})\n",
    "                if extract1 is None and extract2 is None:\n",
    "                    administration = \"\"\n",
    "                elif extract2 is None:\n",
    "                    administration = extract1.get_text(strip = True)\n",
    "                elif extract1 is None:\n",
    "                    administration = extract2.get_text(strip = True)\n",
    "                else:\n",
    "                    administration = extract1.get_text(strip = True) + \" & \" + extract2.get_text(strip = True)\n",
    "\n",
    "            data = [[\"courseName\", \"universityName\", \"facultyName\", \"isItFullTime\", \"description\", \"startDate\", \"fees\", \"modality\", \"duration\", \"city\", \"country\", \"administration\", \"url\"],\n",
    "                    [courseName, universityName, facultyName, isItFullTime, description, startDate, fees, modality, duration, city, country, administration, url]]\n",
    "\n",
    "            with open(tsv_file_path, 'w+', newline='') as tsv_file:\n",
    "                writer = csv.writer(tsv_file, delimiter = '\\t', lineterminator = '\\n')\n",
    "                writer.writerows(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Command Line Question"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As done in the previous assignment, we encourage using the command as a feature that Data Scientists must master.\n",
    "\n",
    "Note: To answer the question in this section, you must strictly use command line tools. We will reject any other method of response. The final script must be placed in CommandLine.sh.\n",
    "\n",
    "First, take the course_i.tsv files you created in point 1 and merge them using Linux commands (Hint: make sure that the first row containing the column names appears only once).\n",
    "\n",
    "Now that you have your merged file named merged_courses.tsv, use Linux commands to answer the following questions:\n",
    "- Which country offers the most Master's Degrees? Which city?\n",
    "- How many colleges offer Part-Time education?\n",
    "- Print the percentage of courses in Engineering (the word \"Engineer\" is contained in the course's name).\n",
    "\n",
    "__Important note:__ You may work on this question in any environment (AWS, your PC command line, Jupyter notebook, etc.), but the final script must be placed in CommandLine.sh, which must be executable. Please run the script and include a __screenshot__ of the <ins>output</ins> in the notebook for evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next cell contains the 'CommandLine.sh' script:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#!/bin/bash\n",
    "\n",
    "\n",
    "#command useful in order to format the output\n",
    "paint=$(tput rev)\n",
    "no_paint=$(tput sgr 0)\n",
    "blue=$(tput setaf 4)\n",
    "red=$(tput setaf 1)\n",
    "green=$(tput setaf 2)\n",
    "yellow=$(tput setaf 3)\n",
    "\n",
    "#printing formatted title and introduction\n",
    "echo -e \"\\n\"\n",
    "echo \"$paint$red                      COMMAND LINE QUESTION HW3 AMDM                            $no_paint\"\n",
    "echo \"$paint$red  $no_paint                                                                            $paint$red  $no_paint\"\n",
    "echo \"$paint$red  $no_paint This bash script merges all the 6000 files .tsv in one and answers to      $paint$red  $no_paint\"\n",
    "echo \"$paint$red  $no_paint the three questions by analysing the .tsv file created.                    $paint$red  $no_paint\"\n",
    "echo \"$paint$red  $no_paint                                                                            $paint$red  $no_paint\"\n",
    "echo \"$paint$red                                                                                $no_paint\"\n",
    "echo -e \"\\n\"\n",
    "echo \"Please wait a few seconds, untill you see the result on standard output, the machine is calculating...\"\n",
    "echo \"For a clearly visualization of the output it's recommended to maximize the terminal window...\"\n",
    "echo -e \"\\n\"\n",
    "\n",
    "\n",
    "#################\n",
    "#               #\n",
    "# Merging files #---------------------------------------------------\n",
    "#               #\n",
    "#################\n",
    "\n",
    "#inizialization of the merged_file with the headers\n",
    "head -n1 course_1.tsv > merged_courses.tsv\n",
    "\n",
    "#appending rows to the merged_file\n",
    "for file in course*.tsv\n",
    "do\n",
    "    tail -n1 $file >> merged_courses.tsv\n",
    "done\n",
    "\n",
    "\n",
    "#################################\n",
    "#                               #\n",
    "# Which country and which city? #-----------------------------------\n",
    "#                               #\n",
    "#################################\n",
    "\n",
    "#assegnation of variable useful for calculate the max\n",
    "max_1=0\n",
    "max_country=' '\n",
    "\n",
    "#extracting the countries column \n",
    "cut -f11 merged_courses.tsv | sed 1d | sort -u > countries.tsv\n",
    "\n",
    "#this command says to the for loop to consider the entire line as a variable \n",
    "IFS=$'\\n'\n",
    "\n",
    "#for loop along all the countries\n",
    "for country in $(cat 'countries.tsv')\n",
    "do\n",
    "    #l contains the occurrence of the country\n",
    "    l=$(cut -f11 merged_courses.tsv | grep -i $country | wc -l)\n",
    "    \n",
    "    #if statement in order to compare and extract the max\n",
    "    if [ $l -ge $max_1 ]\n",
    "    then\n",
    "\tmax_1=$l\n",
    "\tmax_country=$country\n",
    "    fi\n",
    "done\n",
    "\n",
    "#this part works as the previous\n",
    "max_2=0\n",
    "max_city=' '\n",
    "cut -f10 merged_courses.tsv | sed 1d | sort -u > cities.tsv\n",
    "\n",
    "IFS=$'\\n'\n",
    "for city in $(cat 'cities.tsv')\n",
    "do\n",
    "    c=$(cut -f10 merged_courses.tsv | grep -i $city | wc -l)\n",
    "    if [ $c -ge $max_2 ]\n",
    "    then\n",
    "\tmax_2=$c\n",
    "\tmax_city=$city\n",
    "    fi\n",
    "done\n",
    "\n",
    "\n",
    "####################\n",
    "#                   #\n",
    "# Part-time courses #-----------------------------------------------\n",
    "#                   #\n",
    "#####################\n",
    "\n",
    "#extracting the columns of the university name and the time type\n",
    "cut -f2,4 merged_courses.tsv | sed 1d | grep -i 'part time' | cut -f1 > univ_p-t.tsv\n",
    "\n",
    "#sorting and deleting the duplicates we can calculate\n",
    "#the number of university that offers part-time courses\n",
    "num_univ=$(sort -u univ_p-t.tsv | wc -l)\n",
    "\n",
    "\n",
    "##########################################\n",
    "#                                        #\n",
    "# Calculating the courses in Engineering #--------------------------\n",
    "#                                        #\n",
    "##########################################\n",
    "\n",
    "#extracting the column about the courses' name\n",
    "cut -f1 merged_courses.tsv | sed 1d > courseName.tsv\n",
    "\n",
    "#counting the courses with 'Engineer' in their name\n",
    "x=$(grep -i \"Engineer\" courseName.tsv | wc -l)\n",
    "\n",
    "#counting the total courses (not counting the empty lines)\n",
    "y=$(grep -vc '^$' courseName.tsv)\n",
    "\n",
    "#calculating the percentage\n",
    "z=$(echo \"scale=3;$x*100.0/$y\" | bc)\n",
    "\n",
    "\n",
    "#removing the temporary files used for the analysis \n",
    "rm cities.tsv\n",
    "rm countries.tsv\n",
    "rm univ_p-t.tsv\n",
    "rm courseName.tsv\n",
    "\n",
    "#printing formatted output question 1\n",
    "echo \"$paint$blue    QUESTION 1: WHICH COUNTRY OFFERS THE MOST MASTER'S DEGREES? WHICH CITY?     $no_paint\"\n",
    "echo \"$paint$blue  $no_paint                                                                            $paint$blue  $no_paint\"\n",
    "echo \"$paint$blue  $no_paint The country that offers the greater number of Master's Degrees is:         $paint$blue  $no_paint\"\n",
    "echo \"$paint$blue  $no_paint \"$max_country\" with \"$max_1\" courses.                                          $paint$blue  $no_paint\"\n",
    "echo \"$paint$blue  $no_paint The city that offers the greater number of Master's Degree is: \"$max_city\"      $paint$blue  $no_paint\"\n",
    "echo \"$paint$blue  $no_paint with \"$max_2\" courses                                                          $paint$blue  $no_paint\"\n",
    "echo \"$paint$blue  $no_paint                                                                            $paint$blue  $no_paint\"\n",
    "echo \"$paint$blue                                                                                $no_paint\"\n",
    "\n",
    "#question 2\n",
    "echo \"$paint$green    QUESTION 2: HOW MANY COLLEGES OFFER PART-TIME EDUCATION?                    $no_paint\"\n",
    "echo \"$paint$green  $no_paint                                                                            $paint$green  $no_paint\"\n",
    "echo \"$paint$green  $no_paint The number of colleges that offer part-time education is: \"$num_univ\"              $paint$green  $no_paint\"\n",
    "echo \"$paint$green  $no_paint                                                                            $paint$green  $no_paint\"\n",
    "echo \"$paint$green                                                                                $no_paint\"\n",
    "\n",
    "#question 3\n",
    "echo \"$paint$yellow    QUESTION 3: PRINT THE PERCENTAGE OF COURSES IN ENGINEERING                  $no_paint\"\n",
    "echo \"$paint$yellow  $no_paint                                                                            $paint$yellow  $no_paint\"\n",
    "echo \"$paint$yellow  $no_paint The percentage of courses in engineering is: \"$z\"%                       $paint$yellow  $no_paint\"\n",
    "echo \"$paint$yellow  $no_paint                                                                            $paint$yellow  $no_paint\"\n",
    "echo \"$paint$yellow                                                                                $no_paint\"\n",
    "\n",
    "echo -e \"\\n\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "The screenshot below contains the output of the bash script runned on local PC command line using Ubuntu Linux:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![output_screenshot](CLQ_screen.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
