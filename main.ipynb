{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notebook by:\n",
    "* Lorenzo Pannacci 1948926\n",
    "* Francesco Proietti 1873188\n",
    "* Selin Topaloglu 2113300\n",
    "* Santiago Vessi 1958879"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Startup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################\n",
    "# LIBRARIES DOWNLOAD #\n",
    "######################\n",
    "\n",
    "install_packages = False\n",
    "if install_packages:\n",
    "    %pip install beautifulsoup4 tqdm pandas numpy matplotlib nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################\n",
    "# LIBRARIES IMPORT #\n",
    "####################\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from tqdm.notebook import tqdm\n",
    "import time\n",
    "import os\n",
    "import csv\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "import heapq\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data collection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this homework, there is no provided dataset. Instead, you have to build your own. Your search engine will run on text documents. So, here\n",
    "we detail the procedure to follow for the data collection. We strongly suggest you work on different modules when implementing the required functions. For example, you may have a ```crawler.py``` module, a ```parser.py``` module, and a ```engine.py``` module: this is a good practice that improves readability in reporting and efficiency in deploying the code. Be careful; you are likely dealing with exceptions and other possible issues! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. Get the list of master's degree courses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start with the list of courses to include in your corpus of documents. In particular, we focus on web scrapping the [MSc Degrees](https://www.findamasters.com/masters-degrees/msc-degrees/). Next, we want you to **collect the URL** associated with each site in the list from the previously collected list.\n",
    "The list is long and split into many pages. Therefore, we ask you to retrieve only the URLs of the places listed in **the first 400 pages** (each page has 15 courses, so you will end up with 6000 unique master's degree URLs).\n",
    "\n",
    "The output of this step is a `.txt` file whose single line corresponds to the master's URL.\n",
    "\n",
    "---\n",
    "\n",
    "Firstly we observe that we can nagivate trough the different pages using the link `https://www.findamasters.com/masters-degrees/msc-degrees/?PG=n` and changing `n` with the number of the desidered page. We also observe that without waiting between a page load and the other after 30 pages the website thinks we are a bot and don't let us in, to fix this we decided wait some seconds before we open a new page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File already exist and is complete. Using the previous version.\n"
     ]
    }
   ],
   "source": [
    "data_folder_path = r\"data/\"\n",
    "if not os.path.exists(data_folder_path):                    # create main data folder if doesn't already exist\n",
    "    os.makedirs(data_folder_path)\n",
    "\n",
    "courses_urls_path = data_folder_path + r\"courses_urls.txt\"  # file path of the txt file to create\n",
    "sleep_time = 2                                              # idle time between to requests, to avoid being blocke\n",
    "to_crawl = False                                            # we check if the file already exists and has the right length, in this case we do not repeat the crawling\n",
    "n_pages = 400                                               # number of pages to search trough\n",
    "n_courses = n_pages * 15                                    # total number of courses crawled\n",
    "\n",
    "# to avoid the site block the crawler considering it a bot we use a user agent taken from a real chrome session\n",
    "headers = {\"user-agent\": r\"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36\"}\n",
    "\n",
    "if not os.path.exists(courses_urls_path): # if file does not exist we have to crawl\n",
    "    print(\"File does not exist! Crawling...\")\n",
    "    to_crawl = True\n",
    "\n",
    "if to_crawl == False: # if file is incomplete we have to crawl\n",
    "    with open(courses_urls_path, 'r') as file:\n",
    "        file_length = len(file.readlines())\n",
    "        if file_length < n_courses:\n",
    "            print(\"File exist but is incomplete! Crawling...\")\n",
    "            to_crawl = True\n",
    "        else:\n",
    "            print(\"File already exist and is complete. Using the previous version.\")\n",
    "\n",
    "\n",
    "# if data is missing go crawl\n",
    "if to_crawl == True:\n",
    "\n",
    "    with open(\"data/courses_urls.txt\", 'w') as file: # open file, if already exist creates a new one\n",
    "        for i in tqdm(range(n_pages)): # cycle trough every page\n",
    "            url = r\"https://www.findamasters.com/masters-degrees/msc-degrees/?PG=\" + str(1 + i) # we compose the url\n",
    "\n",
    "            # get the webpage\n",
    "            webpage = requests.get(url, headers = headers)\n",
    "            soup = BeautifulSoup(webpage.text)\n",
    "            soup.prettify()\n",
    "\n",
    "            tags = soup.find_all('a', {\"class\": \"courseLink\"})  # get the tags we are interested in\n",
    "\n",
    "            if not tags:\n",
    "                raise IOError(\"Crawler has been blocked by the website. Try again with higher idle time.\")\n",
    "\n",
    "            for tag in tags: # for every tag get the course link and append to file\n",
    "                link = tag[\"href\"]\n",
    "                file.write(r\"https://www.findamasters.com\" + link + \"\\n\")\n",
    "\n",
    "            time.sleep(sleep_time) # wait to avoid getting blocked\n",
    "\n",
    "    # the file automatically close itself when the \"with\" section ends, saving the written lines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Crawl master's degree pages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you get all the URLs in the first 400 pages of the list, you:\n",
    "\n",
    "1. Download the HTML corresponding to each of the collected URLs.\n",
    "2. After you collect a single page, immediately save its `HTML` in a file. In this way, if your program stops for any reason, you will not lose the data collected up to the stopping point.\n",
    "3. Organize the downloaded `HTML` pages into folders. Each folder will contain the `HTML` of the courses on page 1, page 2, ... of the list of master's programs.\n",
    "   \n",
    "__Tip__: Due to the large number of pages you should download, you can use some methods that can help you shorten the time. If you employed a particular process or approach, kindly describe it.\n",
    "\n",
    "---\n",
    "\n",
    "As before we have to insert a idle time between the loading of two pages to avoid that the website block us. We can found whether we have been blocked by checking if the webpage ha the title \"`Just a moment...`\". This makes the operations particularly slow, to crawl all pages we have to wait a few hours. To avoid to repeat this procedure more times than necessary we save the html pages and try to open them only if they are not already downloaded on the device."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All files already crawled. Using the existing version.\n"
     ]
    }
   ],
   "source": [
    "courses_pages_path = r\"data/courses_html_pages/\" # path of the folder containing all the subfolders with the html files\n",
    "\n",
    "# create folder if not exist already\n",
    "if not os.path.exists(courses_pages_path):\n",
    "    os.makedirs(courses_pages_path)\n",
    "\n",
    "# we check if the files already exists, in this case we do not repeat the crawling\n",
    "to_crawl = False\n",
    "files_count = 0\n",
    "for _, _, files in os.walk(courses_pages_path):\n",
    "    files_count += len(files)\n",
    "\n",
    "if files_count < n_courses:\n",
    "    print(\"Crawling...\")\n",
    "    to_crawl = True\n",
    "else:\n",
    "    print(\"All files already crawled. Using the existing version.\")\n",
    "\n",
    "# if data is missing go crawl\n",
    "if to_crawl == True:\n",
    "\n",
    "    # make a folder for every page if not already created\n",
    "    for i in range(1, 400 + 1):\n",
    "        folder_path = courses_pages_path + \"page_\" + str(i)\n",
    "        if not os.path.exists(folder_path):\n",
    "            os.makedirs(folder_path)\n",
    "\n",
    "    # populate folders\n",
    "    with open(courses_urls_path, 'r') as file_1:\n",
    "        for i, course_url in tqdm(enumerate(file_1), total = n_courses):\n",
    "            course_url = course_url.strip('\\n')\n",
    "            course_file_path = courses_pages_path + \"page_\" + str(1 + i // 15) + \"/\" + \"course_\" + str(1 + i % 15) + \".html\"\n",
    "\n",
    "            if not os.path.exists(course_file_path): # if already crawled do not repeat\n",
    "                # get page\n",
    "                webpage = requests.get(course_url, headers = headers)\n",
    "                soup = BeautifulSoup(webpage.text, \"html.parser\")\n",
    "\n",
    "                if soup.title.text == r\"Just a moment...\":\n",
    "                    raise IOError(\"Crawler has been blocked by the website. Try again with higher idle time.\")\n",
    "\n",
    "                # write file\n",
    "                with open(course_file_path, 'w+', encoding = \"utf-8\") as file_2:\n",
    "                    # html_page = soup.prettify()\n",
    "                    # file_2.write(str(html_page))\n",
    "                    file_2.write(str(soup))\n",
    "                # the file automatically close itself when the \"with\" section ends, saving the written lines\n",
    "\n",
    "                time.sleep(sleep_time) # wait to avoid getting blocked"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe that some pages as `https://www.findamasters.com/masters-degrees/course/emergency-management-and-resilience-msc/?i373d7361c25450` (page 215, course 3) are missing and gives us a filler webpage. We will have to treat those courses carefully as the only information we can get from those is the link. We can easily identify those kind of pages thanks to their title: \"`FindAMasters | 500 Error : Internal Server Error`\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# crawling correctness check\n",
    "\n",
    "print(\"Checking the correctness of the crawl operation...\")\n",
    "\n",
    "blocked_pages = 0\n",
    "unaviable_pages = 0\n",
    "correct_pages = 0\n",
    "for root, _, files in tqdm(os.walk(courses_pages_path), total = 401): # checks for files in 400 subfolders and on root folder, thus 401\n",
    "    for file in files:\n",
    "        course_file_path = os.path.join(root, file)\n",
    "\n",
    "        with open(course_file_path, 'r', encoding = \"utf-8\") as html_file:\n",
    "            html_content = html_file.read()\n",
    "\n",
    "        soup = BeautifulSoup(html_content, \"html.parser\")\n",
    "        page_title = soup.title.text\n",
    "\n",
    "        if page_title == r\"Just a moment...\": # blocked during crawling\n",
    "            blocked_pages += 1\n",
    "            os.remove(course_file_path)\n",
    "        elif page_title == r\"FindAMasters | 500 Error : Internal Server Error\": # missing on website\n",
    "            unaviable_pages += 1\n",
    "        else: # downloaded correctly\n",
    "            correct_pages += 1\n",
    "\n",
    "print(blocked_pages, \"pages were blocked during crawling and had been removed. If this value is not zero run the crawling again to get the missing pages.\")\n",
    "print(unaviable_pages, \"pages are not present on the website anymore.\")\n",
    "print(correct_pages, \"pages have been correctly downloaded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Parse downloaded pages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, you should have all the HTML documents about the master's degree of interest, and you can start to extract specific information. The list of the information we desire for each course and their format is as follows:\n",
    "\n",
    "1. Course Name (to save as ```courseName```): string;\n",
    "2. University (to save as ```universityName```): string;\n",
    "3. Faculty (to save as ```facultyName```): string\n",
    "4. Full or Part Time (to save as ```isItFullTime```): string;\n",
    "5. Short Description (to save as ```description```): string;\n",
    "6. Start Date (to save as ```startDate```): string;\n",
    "7. Fees (to save as ```fees```): string;\n",
    "8. Modality (to save as ```modality```):string;\n",
    "9. Duration (to save as ```duration```):string;\n",
    "10. City (to save as ```city```): string;\n",
    "11. Country (to save as ```country```): string;\n",
    "12. Presence or online modality (to save as ```administration```): string;\n",
    "13. Link to the page (to save as ```url```): string.\n",
    "\n",
    "<div style=\"overflow-x:auto;\">\n",
    "<table>\n",
    "<thead>\n",
    "  <tr>\n",
    "    <th>index</th>\n",
    "    <th>courseName</th>\n",
    "    <th>universityName</th>\n",
    "    <th>facultyName</th>\n",
    "    <th>isItFullTime</th>\n",
    "    <th>description</th>\n",
    "    <th>startDate</th>\n",
    "    <th>fees</th>\n",
    "    <th>modality</th>\n",
    "    <th>duration</th>\n",
    "    <th>city</th>\n",
    "    <th>country</th>\n",
    "    <th>administration</th>\n",
    "    <th>url</th>\n",
    "  </tr>\n",
    "</thead>\n",
    "<tbody>\n",
    "  <tr>\n",
    "    <td>0</td>\n",
    "    <td> Accounting and Finance - MSc</td>\n",
    "    <td>University of Leeds</td>\n",
    "    <td>Leeds University Business School</td>\n",
    "    <td>Full time</td>\n",
    "    <td>Businesses and governments rely on [...].</td>\n",
    "    <td>September</td>\n",
    "    <td>UK: £18,000 (Total) International: £34,750 (Total)</td>\n",
    "    <td>MSc</td>\n",
    "    <td>1 year full time</td>\n",
    "    <td>Leeds</td>\n",
    "    <td>United Kingdom</td>\n",
    "    <td>On Campus</td>\n",
    "    <td><a href=\"https://www.findamasters.com/masters-degrees/course/accounting-and-finance-msc/?i321d3232c3891\">Link</a></td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>1</td>\n",
    "    <td> Accounting, Accountability & Financial Management MSc</td>\n",
    "    <td>King’s College London</td>\n",
    "    <td>King’s Business School</td>\n",
    "    <td>Full time</td>\n",
    "    <td>Our Accounting, Accountability & Financial Management MSc course will provide [...].</td>\n",
    "    <td>September</td>\n",
    "    <td>Please see the university website for further information on fees for this course.</td>\n",
    "    <td>MSc</td>\n",
    "    <td>1 year FT</td>\n",
    "    <td>London</td>\n",
    "    <td>United Kingdom</td>\n",
    "    <td>On Campus</td>\n",
    "    <td><a href=\"https://www.findamasters.com/masters-degrees/course/accounting-accountability-and-financial-management-msc/?i132d7816c25522\">Link</a></td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>2</td>\n",
    "    <td> Accounting, Financial Management and Digital Business - MSc</td>\n",
    "    <td>University of Reading</td>\n",
    "    <td>Henley Business School</td>\n",
    "    <td>Full time</td>\n",
    "    <td>Embark on a professional accounting career [...].</td>\n",
    "    <td>September</td>\n",
    "    <td>Please see the university website for further information on fees for this course.</td>\n",
    "    <td>MSc</td>\n",
    "    <td>1 year full time</td>\n",
    "    <td>Reading</td>\n",
    "    <td>United Kingdom</td>\n",
    "    <td>On Campus</td>\n",
    "    <td><a href=\"https://www.findamasters.com/masters-degrees/course/accounting-financial-management-and-digital-business-msc/?i345d4286c351\">Link</a></td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>3</td>\n",
    "    <td> Addictions MSc</td>\n",
    "    <td>King’s College London</td>\n",
    "    <td>Institute of Psychiatry, Psychology and Neuroscience</td>\n",
    "    <td>Full time</td>\n",
    "    <td>Join us for an online session for prospective [...].</td>\n",
    "    <td>September</td>\n",
    "    <td>Please see the university website for further information on fees for this course.</td>\n",
    "    <td>MSc</td>\n",
    "    <td>One year FT</td>\n",
    "    <td>London</td>\n",
    "    <td>United Kingdom</td>\n",
    "    <td>On Campus</td>\n",
    "    <td><a href=\"https://www.findamasters.com/masters-degrees/course/addictions-msc/?i132d4318c27100\">Link</a></td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>4</td>\n",
    "    <td> Advanced Chemical Engineering - MSc</td>\n",
    "    <td>University of Leeds</td>\n",
    "    <td>School of Chemical and Process Engineering</td>\n",
    "    <td>Full time</td>\n",
    "    <td>The Advanced Chemical Engineering MSc at Leeds [...].</td>\n",
    "    <td>September</td>\n",
    "    <td>UK: £13,750 (Total) International: £31,000 (Total)</td>\n",
    "    <td>MSc</td>\n",
    "    <td>1 year full time</td>\n",
    "    <td>Leeds</td>\n",
    "    <td>United Kingdom</td>\n",
    "    <td>On Campus</td>\n",
    "  </tr>\n",
    "  <!-- Add more rows here as needed -->\n",
    "</tbody>\n",
    "</table>\n",
    "</div>\n",
    "\n",
    "\n",
    "For each master's degree, you create a `course_i.tsv` file of this structure:\n",
    "\n",
    "```\n",
    "courseName \\t universityName \\t  ... \\t url\n",
    "```\n",
    "\n",
    "If an information is missing, you just leave it as an empty string.\n",
    "\n",
    "---\n",
    "\n",
    "We can observe that some informations are \"mandatory\", that means that every page (that is not a filler page) has them while others could or could not be present. Meanwhile filler pages gives us no information whatsoever, for those we have only the page url. To create a `.tsv` file we can just use the Python `csv` module changing its delimiter with the character `\\t`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating .tsv files...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ff1cceb7de542fcab6b70f2beb091bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tsvs_path = r\"data/tsvs/\" # path of the folder containing all the .tsv files\n",
    "\n",
    "# create folder if not exist already\n",
    "if not os.path.exists(tsvs_path):\n",
    "    os.makedirs(tsvs_path)\n",
    "\n",
    "# we check if all the files already exists, in this case we do not repeat the crawling\n",
    "to_crawl = False\n",
    "files_count = 0\n",
    "for _, _, files in os.walk(tsvs_path):\n",
    "    files_count += len(files)\n",
    "\n",
    "if files_count < n_courses:\n",
    "    print(\"Creating .tsv files...\")\n",
    "    to_crawl = True\n",
    "else:\n",
    "    print(\"All files already created. Using the existing version.\")\n",
    "\n",
    "# if data is missing go crawl\n",
    "if to_crawl == True:\n",
    "    with open(courses_urls_path, 'r') as courses_file:\n",
    "        for i, url in tqdm(enumerate(courses_file), total = n_courses):\n",
    "            url = url.strip(\"\\n\")\n",
    "\n",
    "            # if file .tsv already exist skip its creation\n",
    "            tsv_file_path = tsvs_path + \"course_\" + str(1 + i) + \".tsv\"\n",
    "            if os.path.exists(tsv_file_path):\n",
    "                continue\n",
    "\n",
    "            # create path, open and read .html file\n",
    "            course_file_path = courses_pages_path + \"page_\" + str(1 + i // 15) + \"/\" + \"course_\" + str(1 + i % 15) + \".html\"\n",
    "            with open(course_file_path, 'r', encoding = \"utf-8\") as html_file:\n",
    "                html_content = html_file.read()\n",
    "\n",
    "            soup = BeautifulSoup(html_content, \"html.parser\")\n",
    "\n",
    "            # if the page is no avaiable we can't get informations\n",
    "            if soup.title.text == r\"FindAMasters | 500 Error : Internal Server Error\":\n",
    "                courseName = universityName = facultyName = isItFullTime = description = startDate = fees = modality = duration = city = country = administration = \"\"\n",
    "\n",
    "            else:\n",
    "                # get all the required fields\n",
    "\n",
    "                courseName = soup.find(\"h1\", {\"class\": \"course-header__course-title\"}).get_text(strip = True)\n",
    "                universityName = soup.find(\"a\", {\"class\": \"course-header__institution\"}).get_text(strip = True)\n",
    "                facultyName = soup.find(\"a\", {\"class\": \"course-header__department\"}).get_text(strip = True)\n",
    "\n",
    "                # some entries do not have this field\n",
    "                extract = soup.find(\"span\", {\"class\": \"key-info__study-type\"})\n",
    "                if extract is None:\n",
    "                    isItFullTime = \"\"\n",
    "                else:\n",
    "                    isItFullTime = extract.get_text(strip = True)\n",
    "\n",
    "                description = soup.find(\"div\", {\"class\": \"course-sections__description\"}).find(\"div\", {\"class\": \"course-sections__content\"}).get_text(strip = True, separator = \" \")\n",
    "                startDate = soup.find(\"span\", {\"class\": \"key-info__start-date\"}).get_text(strip = True)\n",
    "\n",
    "                # some entries do not have this field\n",
    "                extract = soup.find(\"div\", {\"class\": \"course-sections__fees\"})\n",
    "                if extract is None:\n",
    "                    fees = \"\"\n",
    "                else:\n",
    "                    fees = extract.find(\"div\", {\"class\": \"course-sections__content\"}).get_text(strip = True)\n",
    "\n",
    "                modality = soup.find(\"span\", {\"class\": \"key-info__qualification\"}).get_text(strip = True)\n",
    "                duration = soup.find(\"span\", {\"class\": \"key-info__duration\"}).get_text(strip = True)\n",
    "                city = soup.find(\"a\", {\"class\": \"course-data__city\"}).get_text(strip = True)\n",
    "                country = soup.find(\"a\", {\"class\": \"course-data__country\"}).get_text(strip = True)\n",
    "\n",
    "                # courses can be 'on_campus', 'online' or both, but this information is stored in different tags\n",
    "                extract1 = soup.find(\"a\", {\"class\": \"course-data__online\"})\n",
    "                extract2 = soup.find(\"a\", {\"class\": \"course-data__on-campus\"})\n",
    "                if extract1 is None and extract2 is None:\n",
    "                    administration = \"\"\n",
    "                elif extract2 is None:\n",
    "                    administration = extract1.get_text(strip = True)\n",
    "                elif extract1 is None:\n",
    "                    administration = extract2.get_text(strip = True)\n",
    "                else:\n",
    "                    administration = extract1.get_text(strip = True) + \" & \" + extract2.get_text(strip = True)\n",
    "\n",
    "            data = [[\"courseName\", \"universityName\", \"facultyName\", \"isItFullTime\", \"description\", \"startDate\", \"fees\", \"modality\", \"duration\", \"city\", \"country\", \"administration\", \"url\"],\n",
    "                    [courseName, universityName, facultyName, isItFullTime, description, startDate, fees, modality, duration, city, country, administration, url]]\n",
    "\n",
    "            with open(tsv_file_path, 'w+', newline='') as tsv_file:\n",
    "                writer = csv.writer(tsv_file, delimiter = '\\t', lineterminator = '\\n')\n",
    "                writer.writerows(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Search Engine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we want to create two different Search Engines that, given as input a query, return the courses that match the query."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.0 Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.0.0) Preprocessing the text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, you must pre-process all the information collected for each MSc by:\n",
    "\n",
    "1. Removing stopwords\n",
    "2. Removing punctuation\n",
    "3. Stemming\n",
    "4. Anything else you think it's needed\n",
    "   \n",
    "For this purpose, you can use the [`nltk library](https://www.nltk.org/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "The function takes a text string as input and returns a preprocessed version of the text. It utilizes regular expressions to remove all punctuation from the input text. After this punctuation removal, the text is tokenized by splitting it into words. Stopwords, common words that often don't contribute much to the meaning, are then removed from the tokenized text. The function employs stemming using the Porter Stemmer to reduce words to their root form, helping to consolidate similar words. Finally, the preprocessed words are joined back together into a single string, creating the final output.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(txt):\n",
    "    \n",
    "    # remove all punctuation\n",
    "    txt = re.sub(r'[^\\w\\s]', ' ', txt)\n",
    "\n",
    "    # tokenize the text\n",
    "    txt=txt.split()\n",
    "    \n",
    "    # remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    txt = [t for t in txt if t not in stop_words]\n",
    " \n",
    "    # stemming\n",
    "    stemmer = PorterStemmer()\n",
    "    txt = [stemmer.stem(t) for t in txt]\n",
    " \n",
    "    # Reassemble the text\n",
    "    txt = ' '.join(txt)\n",
    "\n",
    "    return txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.0.1) Preprocessing the fees column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Moreover, we want the field ```fees``` to collect numeric information. As you will see, you scraped textual information for this attribute in the dataset: sketch whatever method you need (using regex, for example, to find currency symbol) to collect information and, in case of multiple information, retrieve only the highest fees. Finally, once you have collected numerical information, you likely will have different currencies: this can be chaotic, so let chatGPT guide you in the choice and deployment of an API to convert this column to a common currency of your choice (it can be USD, EUR or whatever you want). Ultimately, you will have a ```float``` column renamed ```fees (CHOSEN COMMON CURRENCY)```."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "This function takes as input a string of text. It starts by finding the currency symbol. The text is previously transformed into its uppercase form to simplify the search. We then search for all possible fees and choose the highest one found. If both are found, we use the Open Exchange Rates API to convert the currency to Euros. Throughout the text, we might find various ways in which the value and currency appear. Sometimes, they will be properly separated by a space bar, while other times both parameters will be joined together. We will consider both of these cases. Also, the value might appear before or after the currency. Finally, for the dollar, pounds, and euros currencies, there are more ways in which they can appear. For this special case, we created a special dictionary that will help translate them into the correct form. So, when we apply the request function to the website, we will have the correct parameters for these currencies.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_eur(fees):\n",
    "    # uppercase the text & tokenize the text \n",
    "    fees=fees.upper().split()\n",
    "    # removes all commas and points\n",
    "    fees=[i.replace(',', '').replace('.', '') for i in fees]\n",
    "\n",
    "    # dict of possible symbols used for dollar, pounds and euros\n",
    "    # in case one of these keys appears we need to change them into their respective values \n",
    "    # to use the request function\n",
    "    sym={'£':'GBP','POUNDS':'GBP','$':'USD','DOLLARS':'USD','€':'EUR','EUROS':'EUR'}\n",
    "    # list of all the possible currencies\n",
    "    curr=[\"£\",\"POUNDS\",\"GBP\",\"$\",\"DOLLARS\",\"USD\",\"€\",\"EUROS\",\"EUR\",\"AED\",\"AFN\",\"ALL\",\"AMD\",\"ANG\",\"AOA\",\"ARS\",\"AUD\",\"AWG\",\"AZN\",\"BAM\",\"BBD\",\"BDT\",\"BGN\",\"BHD\",\"BIF\",\"BMD\",\"BND\",\"BOB\",\"BRL\",\"BSD\",\"BTN\",\"BWP\",\"BYN\",\"BZD\",\"CAD\",\"CDF\",\"CHF\",\"CLP\",\"CNY\",\"COP\",\"CRC\",\"CUP\",\"CVE\",\"CZK\",\"DJF\",\"DKK\",\"DOP\",\"DZD\",\"EGP\",\"ERN\",\"ETB\",\"FJD\",\"FKP\",\"FOK\",\"GEL\",\"GGP\",\"GHS\",\"GIP\",\"GMD\",\"GNF\",\"GTQ\",\"GYD\",\"HKD\",\"HNL\",\"HRK\",\"HTG\",\"HUF\",\"IDR\",\"ILS\",\"IMP\",\"INR\",\"IQD\",\"IRR\",\"ISK\",\"JEP\",\"JMD\",\"JOD\",\"JPY\",\"KES\",\"KGS\",\"KHR\",\"KID\",\"KMF\",\"KRW\",\"KWD\",\"KYD\",\"KZT\",\"LAK\",\"LBP\",\"LKR\",\"LRD\",\"LSL\",\"LYD\",\"MAD\",\"MDL\",\"MGA\",\"MKD\",\"MMK\",\"MNT\",\"MOP\",\"MRU\",\"MUR\",\"MVR\",\"MWK\",\"MXN\",\"MYR\",\"MZN\",\"NAD\",\"NGN\",\"NIO\",\"NOK\",\"NPR\",\"NZD\",\"OMR\",\"PAB\",\"PEN\",\"PGK\",\"PHP\",\"PKR\",\"PLN\",\"PYG\",\"QAR\",\"RON\",\"RSD\",\"RUB\",\"RWF\",\"SAR\",\"SBD\",\"SCR\",\"SDG\",\"SEK\",\"SGD\",\"SHP\",\"SLE\",\"SLL\",\"SOS\",\"SRD\",\"SSP\",\"STN\",\"SYP\",\"SZL\",\"THB\",\"TJS\",\"TMT\",\"TND\",\"TOP\",\"TRY\",\"TTD\",\"TVD\",\"TWD\",\"TZS\",\"UAH\",\"UGX\",\"UYU\",\"UZS\",\"VES\",\"VND\",\"VUV\",\"WST\",\"XAF\",\"XCD\",\"XDR\",\"XOF\",\"XPF\",\"YER\",\"ZAR\",\"ZMW\",\"ZWL\"]\n",
    "    \n",
    "    # find value and currency\n",
    "    v=[] # list of values\n",
    "    c=[] # list of currencies\n",
    "    for i,elem in enumerate(fees):\n",
    "        if elem.isdigit():\n",
    "            # check if there is a currency used before the digit\n",
    "            if i>0 and fees[i-1] in curr:\n",
    "                v.append(float(elem))\n",
    "                if (fees[i-1] in sym):\n",
    "                    c.append(sym[fees[i-1]])\n",
    "                else:\n",
    "                    c.append(fees[i-1])\n",
    "            elif i<len(fees)-1 and fees[i+1] in curr:\n",
    "                # check if there is a currency used after the digit\n",
    "                v.append(float(elem))\n",
    "                if (fees[i+1] in sym): \n",
    "                    c.append(sym[fees[i+1]])\n",
    "                else:\n",
    "                    c.append(fees[i+1])\n",
    "        # check if the currency and the value are attached together as a same element\n",
    "        elif len(elem)>0 and elem[len(elem)-1] in sym:\n",
    "            val = re.findall(r\"\\d+\",elem[:len(elem)])\n",
    "            if val:\n",
    "                v.append(float(val[0]))\n",
    "                c.append(sym[elem[len(elem)-1]])\n",
    "        elif len(elem)>0 and elem[0] in sym:\n",
    "            val = re.findall(r\"\\d+\",elem[1:len(elem)])\n",
    "            if val:\n",
    "                v.append(float(val[0]))\n",
    "                c.append(sym[elem[0]])\n",
    "\n",
    "    conv=\"0.0\"\n",
    "    \n",
    "    if len(c)>0:\n",
    "        base_url = \"https://open.er-api.com/v6/latest\"\n",
    "        API_KEY=\"6d56fb10262e4f29bef560d4c38fa3f4\"\n",
    "        conv=[]\n",
    "        # convert all the values into euros \n",
    "        for i in range(len(v)):\n",
    "            params = {'base': c[i], 'apiKey': API_KEY}\n",
    "            response = requests.get(base_url, params=params)\n",
    "            rates = response.json().get('rates', {})\n",
    "            conv.append(np.round(v[i] / rates['EUR'], 2))\n",
    "        # get the maximum value\n",
    "        conv=max(conv)\n",
    "    return conv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1, 6001):\n",
    "    tsv = \"course_\" + str(i) + \".tsv\"\n",
    "    file_path = os.path.join(tsvs_path, tsv)\n",
    "    with open(file_path, 'r', encoding='utf-8') as ff:\n",
    "        lines=ff.readlines()\n",
    "    \n",
    "    fields = lines[1].strip().split('\\t')\n",
    "    \n",
    "    # Ensure that the list has enough elements\n",
    "    if len(fields) > 6:\n",
    "        \n",
    "        # get the fee value\n",
    "        f=convert_to_eur(fields[6])\n",
    "    \n",
    "        # Read the TSV file into a DataFrame\n",
    "        df = pd.read_csv(file_path, sep='\\t')\n",
    "    \n",
    "        # add a new column in the eighth position\n",
    "        df.insert(7, 'fees (EUR)', f)\n",
    "\n",
    "        ## Save the modified DataFrame back to the same TSV file\n",
    "        df.to_csv(file_path, sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. Conjunctive query"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the first version of the search engine, we narrowed our interest to the __description__ of each course. It means that you will evaluate queries only concerning the course's description."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.1) Create your index!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before building the index, \n",
    "* Create a file named `vocabulary`, in the format you prefer, that maps each word to an integer (`term_id`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "To create the vocabulary, we begin by initializing an empty dictionary. For each of the 6000 tsv files, we extract the description field. After preprocessing the text, we iterate through each word, checking if it's already in the vocabulary. If not, we add it and assign a unique ID. Finally, the vocabulary is saved in a txt file.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start an empty dictionary\n",
    "vocabulary = {}\n",
    "# start with term_id as 1\n",
    "term_id = 1\n",
    "\n",
    "for i in range(1,6001):\n",
    "    tsv=\"course_\"+str(i)+\".tsv\"\n",
    "    file_path= os.path.join(tsvs_path,tsv)\n",
    "    with open(file_path, 'r', encoding='utf-8') as ff:\n",
    "        lines=ff.readlines()\n",
    "    \n",
    "    fields = lines[1].strip().split('\\t')\n",
    "    # Ensure that the list has enough elements\n",
    "    if len(fields) > 4:\n",
    "        \n",
    "        # preprocess the description\n",
    "        d=preprocess_text(fields[4])\n",
    "        # tokenize \n",
    "        words=d.split()\n",
    "        \n",
    "        for w in words:\n",
    "            if w not in vocabulary: #add to the vocabulary if the word is not in there yet\n",
    "                vocabulary[w]=term_id #assigns a unique id\n",
    "                term_id+=1 # update term_id\n",
    "\n",
    "# save the vocabulary to a file\n",
    "vocabulary_file_path = r\"data/vocabulary.txt\"\n",
    "with open(vocabulary_file_path, 'w', encoding='utf-8') as vocab:\n",
    "    for word, term_id in vocabulary.items():\n",
    "        vocab.write(f\"{word}\\t{term_id}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, the first brick of your homework is to create the Inverted Index. It will be a dictionary in this format:\n",
    "\n",
    "```\n",
    "{\n",
    "term_id_1:[document_1, document_2, document_4],\n",
    "term_id_2:[document_1, document_3, document_5, document_6],\n",
    "...}\n",
    "```\n",
    "where _document\\_i_ is the *id* of a document that contains that specific word.\n",
    "\n",
    "__Hint:__ Since you do not want to compute the inverted index every time you use the Search Engine, it is worth thinking about storing it in a separate file and loading it in memory when needed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "An empty dictionary, inverted_index, is created to store the inverted index, where each *term_ID* points to a list of documents containing that term. For each tsv file, we check if there are more than 4 fields to ensure the presence of a description field. The description field is then preprocessed using the preprocess_text function, and the resulting text is tokenized into words. For each word in the tokenized description:\n",
    "\n",
    "- Check if the word contains alphabetical characters or numbers.\n",
    "- If the word is in the vocabulary, update the inverted index:\n",
    "- - Obtain the term ID for the word from the vocabulary.\n",
    "- - Check if the term ID is already in the inverted index. If not, create a new entry with the tsv file.\n",
    "- - If the term ID is already in the inverted index, add the document to the existing list, but only if it's not already present.\n",
    "\n",
    "After processing all course descriptions, the inverted index is saved to a file (inv_index_file_path) with each line containing a term ID and the list of documents containing that term, separated by a tab.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize an empty inverted_index\n",
    "inverted_index={}\n",
    "\n",
    "for i in range(1,6001):\n",
    "    tsv=\"course_\"+str(i)+\".tsv\"\n",
    "    file_path= os.path.join(tsvs_path,tsv)\n",
    "    with open(file_path, 'r', encoding='utf-8') as ff:\n",
    "        lines=ff.readlines()\n",
    "    \n",
    "    fields = lines[1].strip().split('\\t')\n",
    "    # Ensure that the list has enough elements\n",
    "    if len(fields) > 4:\n",
    "        # access the description field and tokenize the words,\n",
    "        d=preprocess_text(fields[4])\n",
    "        words=d.split()\n",
    "        for w in words:\n",
    "            # check if the word contains alphabetical characters or numbers\n",
    "            if w in vocabulary:\n",
    "                # update inverted index\n",
    "                id_term = vocabulary[w]\n",
    "                if id_term not in inverted_index:\n",
    "                    inverted_index[id_term]=[tsv]\n",
    "                else:\n",
    "                    if(tsv not in inverted_index[id_term]):\n",
    "                        inverted_index[id_term].append(tsv)\n",
    "\n",
    "\n",
    "# save the inverted index in a file\n",
    "inv_index_file_path = r\"data/inverted_index.txt\"\n",
    "with open(inv_index_file_path, 'w', encoding='utf-8') as inv_ind:\n",
    "    for word, term_id in inverted_index.items():\n",
    "        inv_ind.write(f\"{word}\\t{term_id}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.2) Execute the query\n",
    "Given a query input by the user, for example:\n",
    "\n",
    "```\n",
    "advanced knowledge\n",
    "```\n",
    "\n",
    "The Search Engine is supposed to return a list of documents.\n",
    "\n",
    "##### What documents do we want?\n",
    "Since we are dealing with conjunctive queries (AND), each returned document should contain all the words in the query.\n",
    "The final output of the query must return, if present, the following information for each of the selected documents:\n",
    "\n",
    "* `courseName`\n",
    "* `universityName`\n",
    "* `description`\n",
    "* `URL`\n",
    "\n",
    "If everything works well in this step, you can go to the next point and make your Search Engine more complex and better at answering queries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "The input query is preprocessed using the preprocess_text function, and the resulting text is tokenized into individual words. The variable *doc* is initialized as an empty set. It will eventually contain all the documents that have the complete query in their description.\n",
    "The code iterates through each word in the processed query:\n",
    "- For each word, it checks if the word is in the vocabulary. If so, it retrieves the term ID.\n",
    "- If the term ID is in the inverted index, it collects the set of documents associated with that term ID.\n",
    "- If it's the first word in the query, it updates the doc set. Otherwise, it updates a temporary set tmp.\n",
    "- It then filters out the documents that don't contain the previous word of the query (for words beyond the first).\n",
    "\n",
    "For each document ID in the final doc set, it reads the corresponding tsv file.\n",
    "Relevant information from the file, such as course name, university name, description, and URL, is extracted and stored in a list of dictionaries (*result_data*).\n",
    "The extracted information is used to create a Pandas DataFrame (result_df), which is then returned by the function.\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_engine(query):\n",
    "    query_words = preprocess_text(query).split()\n",
    "    # this list will contain all the docs that have the complete query in their description\n",
    "    doc = set() \n",
    "    for i in range(len(query_words)):\n",
    "        tmp=set() # we will need this to determine if a document contains all the query elements\n",
    "        w = query_words[i]\n",
    "        if w in vocabulary:\n",
    "            term_id = vocabulary[w]\n",
    "            if term_id in inverted_index:\n",
    "                t_id=inverted_index[term_id]\n",
    "                if i==0:\n",
    "                    doc.update(t_id)\n",
    "                else:\n",
    "                    tmp.update(t_id)\n",
    "       # filters out the documents that don't contain the previous word of the query\n",
    "        if i>0:\n",
    "            doc=doc.intersection(doc, tmp)\n",
    "                    \n",
    "\n",
    "    # Extract information from matching documents\n",
    "    result_data = []\n",
    "    for tsv_id in doc:\n",
    "        tsv_file = os.path.join(tsvs_path, f\"{tsv_id}\")\n",
    "        with open(tsv_file, 'r', encoding='utf-8') as ff:\n",
    "            lines=ff.readlines()\n",
    "        fields=lines[1].split(\"\\t\")\n",
    "        result_data.append({\n",
    "                    'courseName': fields[0],\n",
    "                    'universityName': fields[1],\n",
    "                    'description': fields[4],\n",
    "                    'URL': fields[-1],\n",
    "        })\n",
    "\n",
    "    # Create pandas DataFrame\n",
    "    result_df = pd.DataFrame(result_data)\n",
    "    return result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['advanced', 'knowledge']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: []\n",
       "Index: []"
      ]
     },
     "execution_count": 278,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q=\"advanced knowledge\"\n",
    "search_engine(q)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2) Conjunctive query & Ranking score\n",
    "For the second search engine, given a query, we want to get the top-k (the choice of k it's up to you!) documents related to the query. In particular:\n",
    "\n",
    "- Find all the documents that contain all the words in the query.\n",
    "- Sort them by their similarity with the query.\n",
    "- Return in output *k* documents, or all the documents with non-zero similarity with the query when the results are less than *k*. You must use a heap data structure (you can use Python libraries) for maintaining the *top-k* documents.\n",
    "\n",
    "To solve this task, you must use the *tfIdf* score and the *Cosine similarity*. The field to consider is still the description. Let's see how.\n",
    "\n",
    "#### 2.2.1) Inverted index\n",
    "Your second Inverted Index must be of this format:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "{\n",
    "term_id_1:[(document1, tfIdf_{term,document1}), (document2, tfIdf_{term,document2}), (document4, tfIdf_{term,document4}), ...],\n",
    "term_id_2:[(document1, tfIdf_{term,document1}), (document3, tfIdf_{term,document3}), (document5, tfIdf_{term,document5}), (document6, tfIdf_{term,document6}), ...],\n",
    "...}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Practically, for each word, you want the list of documents in which it is contained and the relative tfIdf score.\n",
    "\n",
    "Tip: TfIdf values are invariant for the query. Due to this reason, you can precalculate and store them accordingly.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "The code initializes an inverted index *inv_index_tfid* and dictionaries for term frequency *t_f* and document frequency *d_f*.\n",
    "The code iterates through a range of document IDs and reads the corresponding TSV files. \n",
    "Term Frequency Calculation:\n",
    "- The code iterates through each word in the list of words extracted from the document description (preprocessed).\n",
    "- It retrieves the term ID for each word from the vocabulary.\n",
    "- If the term ID exists it updates the term frequency dictionary (t_f) for the pair (term_id, tsv) by incrementing the count.\n",
    "\n",
    "Document Frequency (DF) Calculation:\n",
    "- The code initializes a set to keep track of unique terms encountered in the document.\n",
    "- It then iterates through the set of unique words in the document.\n",
    "- For each word, it retrieves the term ID from the vocabulary.\n",
    "- If the term ID exists, and it hasn't been seen before in the document, it updates the document frequency dictionary for the term ID by incrementing the count. It also adds the term ID to the set of seen words.\n",
    "\n",
    "Then the TF-IDF score is calculated and rounded to two decimal places. Finally it updates, the inverted index with the term ID and a tuple containg the ID and it's TF-IDF score.\n",
    " \n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the inverted index with tf-idf scores \n",
    "inv_index_tfidf = {}\n",
    "# Initialize dictionaries for term frequency (t_f) and document frequency (d_f)\n",
    "t_f = {}\n",
    "d_f = {}\n",
    "\n",
    "# Step 1: Calculate term frequency (tf) and inverse document frequency (idf)\n",
    "for i in range(1,6001):\n",
    "    tsv=\"course_\"+str(i)+\".tsv\"\n",
    "    file_path= os.path.join(tsvs_path,tsv)\n",
    "    with open(file_path, 'r', encoding='utf-8') as ff:\n",
    "        lines=ff.readlines()\n",
    "    \n",
    "    # Extract the fields from the second line\n",
    "    fields = lines[1].strip().split('\\t')\n",
    "    \n",
    "    # Ensure that the list has enough elements\n",
    "    if len(fields) > 4:\n",
    "        # access the description field and tokenize the words, \n",
    "        words=preprocess_text(fields[4]).split()\n",
    "\n",
    "        # calculate term frequency (tf) for each term in the document\n",
    "        for w in words:\n",
    "            term_id = vocabulary.get(w)\n",
    "            if term_id:\n",
    "                # if the pair doesn't exist  it is initialized with a count of 0 before incrementing.\n",
    "                # If the pair already exists, it doesn't override the existing value; \n",
    "                # it simply returns the existing value associated with that key.\n",
    "                t_f.setdefault((term_id, tsv), 0) \n",
    "                t_f[(term_id, tsv)] += 1\n",
    "\n",
    "        # update document frequency for each term\n",
    "        seen_words = set()\n",
    "        for word in set(words):\n",
    "            term_id = vocabulary.get(word)\n",
    "            if term_id and term_id not in seen_words:\n",
    "                d_f.setdefault(term_id, 0)\n",
    "                d_f[term_id] += 1\n",
    "                seen_words.add(term_id)\n",
    "\n",
    "# step 2: calculate tf-idf and build the inverted index\n",
    "for (term_id, doc_id),tf in t_f.items():\n",
    "    \n",
    "    # calculate inverse document frequency (idf)\n",
    "    idf = np.log(6000/ (d_f[term_id] + 1))  \n",
    "    # calculate tf-idf score\n",
    "    tfidf = np.round(tf * idf,2)\n",
    "    \n",
    "    # update the inverted index with the term_id and the corresponding tuple\n",
    "    inv_index_tfidf.setdefault(term_id, [])\n",
    "    inv_index_tfidf[term_id].append((doc_id, tfidf))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the inverted index in a file\n",
    "inv_ind_tfidf_file_path = r\"data/inv_index_tfidf.txt\"\n",
    "with open(inv_ind_tfidf_file_path, 'w', encoding='utf-8') as tfidf:\n",
    "    for word, term_id in inv_index_tfidf.items():\n",
    "        tfidf.write(f\"{word}\\t{term_id}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.2) Execute the query\n",
    "In this new setting, given a query, you get the proper documents (i.e., those containing all the query's words) and sort them according to their similarity to the query. For this purpose, as the scoring function, we will use the Cosine Similarity concerning the tfIdf representations of the documents.\n",
    "\n",
    "Given a query input by the user, for example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "advanced knowledge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The search engine is supposed to return a list of documents, ranked by their Cosine Similarity to the query entered in the input.\n",
    "\n",
    "More precisely, the output must contain:\n",
    "\n",
    "- courseName\n",
    "- universityName\n",
    "- description\n",
    "- URL\n",
    "- The similarity score of the documents with respect to the query (float value between 0 and 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "The function starts by preprocessing the query and vectorizing it using the TfidfVectorizer function, calculating the TF-IDF score for each term. Following this, it identifies documents that contain all the query words in their description, leveraging the same method as the search_engine function. Afterward, the function computes the cosine similarity for each document. This is achieved using the query vector and the document vector, where the document vector is constructed from the TF-IDF scores in the inverted index file. The function employs a heap data structure to efficiently maintain the top documents based on their cosine similarity scores. In cases where there are fewer than k documents satisfying the query conditions, the function returns all available documents.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_k_documents(query,k=5):\n",
    "    # Preprocess and tokenize the query\n",
    "    query_words = preprocess_text(query)\n",
    "    \n",
    "    # vectorize the query\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    X = vectorizer.fit_transform([query_words])\n",
    "    query_vector = X.toarray()[0]\n",
    "\n",
    "    # tokenize the query\n",
    "    query_words=query_words.split()\n",
    "    \n",
    "    # Find documents that contain all words in the query\n",
    "    doc = set()\n",
    "    for i in range(len(query_words)):\n",
    "        tmp=set() \n",
    "        w = query_words[i]\n",
    "        if w in vocabulary:\n",
    "            term_id = vocabulary[w]\n",
    "            if term_id in inverted_index:\n",
    "                t_id=inverted_index[term_id]\n",
    "                if i==0:\n",
    "                    doc.update(t_id)\n",
    "                else:\n",
    "                    tmp.update(t_id)\n",
    "        if i>0:\n",
    "            doc=doc.intersection(doc, tmp)        \n",
    "    \n",
    "\n",
    "    # Calculate cosine similarity for each matching document\n",
    "    heap = []\n",
    "    for doc_id in doc:\n",
    "        doc_vector = {}\n",
    "        # Aggregate tf-idf scores for each term in the document\n",
    "        for term_id, tfidf in inv_index_tfidf.items():\n",
    "            for doc, score in tfidf:\n",
    "                if doc == doc_id:\n",
    "                    doc_vector[term_id] = score\n",
    "\n",
    "        # calculate the cosine similarity\n",
    "        prod = 0.0\n",
    "        for i in range(len(query_vector)):\n",
    "            prod += query_vector[i] * doc_vector[vocabulary[query_words[i]]] \n",
    "        norm_doc = np.linalg.norm(np.array(list(doc_vector.values()))) \n",
    "        norm_query = np.linalg.norm(query_vector) \n",
    "        if norm_doc != 0 and norm_query != 0:\n",
    "            score = prod / (norm_doc * norm_query)\n",
    "        \n",
    "        # Add the document information and similarity score to the heap\n",
    "        heapq.heappush(heap, (-score, doc_id))\n",
    "\n",
    "        \n",
    "        \n",
    "    # Get the top-k documents \n",
    "    result_documents = []\n",
    "    for i in range(min(k,len(heap))):\n",
    "        similarity_score, doc_id = heapq.heappop(heap)\n",
    "        \n",
    "        tsv_file = os.path.join(tsvs_path, f\"{doc_id}\")\n",
    "        with open(tsv_file, 'r', encoding='utf-8') as ff:\n",
    "            lines=ff.readlines()\n",
    "        fields=lines[1].split(\"\\t\")\n",
    "        result_documents.append({\n",
    "                    'courseName': fields[0],\n",
    "                    'universityName': fields[1],\n",
    "                    'description': fields[4],\n",
    "                    'URL': fields[-1],\n",
    "                    'similarityScore': -similarity_score  # Convert back to positive\n",
    "        })\n",
    "        \n",
    "    # return pandas DataFrame\n",
    "    return pd.DataFrame(result_documents)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>courseName</th>\n",
       "      <th>universityName</th>\n",
       "      <th>description</th>\n",
       "      <th>URL</th>\n",
       "      <th>similarityScore</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Advanced Computing MSc</td>\n",
       "      <td>King’s College London</td>\n",
       "      <td>Our Advanced Computing MSc provides knowledge ...</td>\n",
       "      <td>https://www.findamasters.com/masters-degrees/c...</td>\n",
       "      <td>0.324157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Advanced Healthcare Practice - MSc</td>\n",
       "      <td>Cardiff University</td>\n",
       "      <td>Why study this course Our MSc Advanced Healthc...</td>\n",
       "      <td>https://www.findamasters.com/masters-degrees/c...</td>\n",
       "      <td>0.313722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Advanced Clinical Practice MSc</td>\n",
       "      <td>University of Greenwich</td>\n",
       "      <td>Learn essential strategies and prepare for lea...</td>\n",
       "      <td>https://www.findamasters.com/masters-degrees/c...</td>\n",
       "      <td>0.309663</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Advancing Practice - MSc</td>\n",
       "      <td>University of Northampton</td>\n",
       "      <td>Our MSc Advancing Practice awards support the ...</td>\n",
       "      <td>https://www.findamasters.com/masters-degrees/c...</td>\n",
       "      <td>0.304063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Advanced Clinical Practice - MSc</td>\n",
       "      <td>Canterbury Christ Church University</td>\n",
       "      <td>Gain the knowledge and skills needed to become...</td>\n",
       "      <td>https://www.findamasters.com/masters-degrees/c...</td>\n",
       "      <td>0.302410</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           courseName                       universityName  \\\n",
       "0              Advanced Computing MSc                King’s College London   \n",
       "1  Advanced Healthcare Practice - MSc                   Cardiff University   \n",
       "2      Advanced Clinical Practice MSc              University of Greenwich   \n",
       "3            Advancing Practice - MSc            University of Northampton   \n",
       "4    Advanced Clinical Practice - MSc  Canterbury Christ Church University   \n",
       "\n",
       "                                         description  \\\n",
       "0  Our Advanced Computing MSc provides knowledge ...   \n",
       "1  Why study this course Our MSc Advanced Healthc...   \n",
       "2  Learn essential strategies and prepare for lea...   \n",
       "3  Our MSc Advancing Practice awards support the ...   \n",
       "4  Gain the knowledge and skills needed to become...   \n",
       "\n",
       "                                                 URL  similarityScore  \n",
       "0  https://www.findamasters.com/masters-degrees/c...         0.324157  \n",
       "1  https://www.findamasters.com/masters-degrees/c...         0.313722  \n",
       "2  https://www.findamasters.com/masters-degrees/c...         0.309663  \n",
       "3  https://www.findamasters.com/masters-degrees/c...         0.304063  \n",
       "4  https://www.findamasters.com/masters-degrees/c...         0.302410  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "top_k_documents(\"advanced knowledge\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Command Line Question"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As done in the previous assignment, we encourage using the command as a feature that Data Scientists must master.\n",
    "\n",
    "Note: To answer the question in this section, you must strictly use command line tools. We will reject any other method of response. The final script must be placed in CommandLine.sh.\n",
    "\n",
    "First, take the course_i.tsv files you created in point 1 and merge them using Linux commands (Hint: make sure that the first row containing the column names appears only once).\n",
    "\n",
    "Now that you have your merged file named merged_courses.tsv, use Linux commands to answer the following questions:\n",
    "- Which country offers the most Master's Degrees? Which city?\n",
    "- How many colleges offer Part-Time education?\n",
    "- Print the percentage of courses in Engineering (the word \"Engineer\" is contained in the course's name).\n",
    "\n",
    "__Important note:__ You may work on this question in any environment (AWS, your PC command line, Jupyter notebook, etc.), but the final script must be placed in CommandLine.sh, which must be executable. Please run the script and include a __screenshot__ of the <ins>output</ins> in the notebook for evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next cell contains the 'CommandLine.sh' script:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "\n",
    "#!/bin/bash\n",
    "\n",
    "\n",
    "#command useful in order to format the output\n",
    "paint=$(tput rev)\n",
    "no_paint=$(tput sgr 0)\n",
    "blue=$(tput setaf 4)\n",
    "red=$(tput setaf 1)\n",
    "green=$(tput setaf 2)\n",
    "yellow=$(tput setaf 3)\n",
    "\n",
    "#printing formatted title and introduction\n",
    "echo -e \"\\n\"\n",
    "echo \"$paint$red                      COMMAND LINE QUESTION HW3 AMDM                            $no_paint\"\n",
    "echo \"$paint$red  $no_paint                                                                            $paint$red  $no_paint\"\n",
    "echo \"$paint$red  $no_paint This bash script merges all the 6000 files .tsv in one and answers to      $paint$red  $no_paint\"\n",
    "echo \"$paint$red  $no_paint the three questions by analysing the .tsv file created.                    $paint$red  $no_paint\"\n",
    "echo \"$paint$red  $no_paint                                                                            $paint$red  $no_paint\"\n",
    "echo \"$paint$red                                                                                $no_paint\"\n",
    "echo -e \"\\n\"\n",
    "echo \"Please wait a few seconds, untill you see the result on standard output, the machine is calculating...\"\n",
    "echo \"For a clearly visualization of the output it's recommended to maximize the terminal window...\"\n",
    "echo -e \"\\n\"\n",
    "\n",
    "\n",
    "#################\n",
    "#               #\n",
    "# Merging files #---------------------------------------------------\n",
    "#               #\n",
    "#################\n",
    "\n",
    "#inizialization of the merged_file with the headers\n",
    "head -n1 course_1.tsv > merged_courses.tsv\n",
    "\n",
    "#appending rows to the merged_file\n",
    "for file in course*.tsv\n",
    "do\n",
    "    tail -n1 $file >> merged_courses.tsv\n",
    "done\n",
    "\n",
    "\n",
    "#################################\n",
    "#                               #\n",
    "# Which country and which city? #-----------------------------------\n",
    "#                               #\n",
    "#################################\n",
    "\n",
    "#assegnation of variable useful for calculate the max\n",
    "max_1=0\n",
    "max_country=' '\n",
    "\n",
    "#extracting the countries column \n",
    "cut -f11 merged_courses.tsv | sed 1d | sort -u > countries.tsv\n",
    "\n",
    "#this command says to the for loop to consider the entire line as a variable \n",
    "IFS=$'\\n'\n",
    "\n",
    "#for loop along all the countries\n",
    "for country in $(cat 'countries.tsv')\n",
    "do\n",
    "    #l contains the occurrence of the country\n",
    "    l=$(cut -f11 merged_courses.tsv | grep -i $country | wc -l)\n",
    "    \n",
    "    #if statement in order to compare and extract the max\n",
    "    if [ $l -ge $max_1 ]\n",
    "    then\n",
    "\tmax_1=$l\n",
    "\tmax_country=$country\n",
    "    fi\n",
    "done\n",
    "\n",
    "#this part works as the previous\n",
    "max_2=0\n",
    "max_city=' '\n",
    "cut -f10 merged_courses.tsv | sed 1d | sort -u > cities.tsv\n",
    "\n",
    "IFS=$'\\n'\n",
    "for city in $(cat 'cities.tsv')\n",
    "do\n",
    "    c=$(cut -f10 merged_courses.tsv | grep -i $city | wc -l)\n",
    "    if [ $c -ge $max_2 ]\n",
    "    then\n",
    "\tmax_2=$c\n",
    "\tmax_city=$city\n",
    "    fi\n",
    "done\n",
    "\n",
    "\n",
    "####################\n",
    "#                   #\n",
    "# Part-time courses #-----------------------------------------------\n",
    "#                   #\n",
    "#####################\n",
    "\n",
    "#extracting the columns of the university name and the time type\n",
    "cut -f2,4 merged_courses.tsv | sed 1d | grep -i 'part time' | cut -f1 > univ_p-t.tsv\n",
    "\n",
    "#sorting and deleting the duplicates we can calculate\n",
    "#the number of university that offers part-time courses\n",
    "num_univ=$(sort -u univ_p-t.tsv | wc -l)\n",
    "\n",
    "\n",
    "##########################################\n",
    "#                                        #\n",
    "# Calculating the courses in Engineering #--------------------------\n",
    "#                                        #\n",
    "##########################################\n",
    "\n",
    "#extracting the column about the courses' name\n",
    "cut -f1 merged_courses.tsv | sed 1d > courseName.tsv\n",
    "\n",
    "#counting the courses with 'Engineer' in their name\n",
    "x=$(grep -i \"Engineer\" courseName.tsv | wc -l)\n",
    "\n",
    "#counting the total courses (not counting the empty lines)\n",
    "y=$(grep -vc '^$' courseName.tsv)\n",
    "\n",
    "#calculating the percentage\n",
    "z=$(echo \"scale=3;$x*100.0/$y\" | bc)\n",
    "\n",
    "\n",
    "#removing the temporary files used for the analysis \n",
    "rm cities.tsv\n",
    "rm countries.tsv\n",
    "rm univ_p-t.tsv\n",
    "rm courseName.tsv\n",
    "\n",
    "#printing formatted output question 1\n",
    "echo \"$paint$blue    QUESTION 1: WHICH COUNTRY OFFERS THE MOST MASTER'S DEGREES? WHICH CITY?     $no_paint\"\n",
    "echo \"$paint$blue  $no_paint                                                                            $paint$blue  $no_paint\"\n",
    "echo \"$paint$blue  $no_paint The country that offers the greater number of Master's Degrees is:         $paint$blue  $no_paint\"\n",
    "echo \"$paint$blue  $no_paint \"$max_country\" with \"$max_1\" courses.                                          $paint$blue  $no_paint\"\n",
    "echo \"$paint$blue  $no_paint The city that offers the greater number of Master's Degree is: \"$max_city\"      $paint$blue  $no_paint\"\n",
    "echo \"$paint$blue  $no_paint with \"$max_2\" courses                                                          $paint$blue  $no_paint\"\n",
    "echo \"$paint$blue  $no_paint                                                                            $paint$blue  $no_paint\"\n",
    "echo \"$paint$blue                                                                                $no_paint\"\n",
    "\n",
    "#question 2\n",
    "echo \"$paint$green    QUESTION 2: HOW MANY COLLEGES OFFER PART-TIME EDUCATION?                    $no_paint\"\n",
    "echo \"$paint$green  $no_paint                                                                            $paint$green  $no_paint\"\n",
    "echo \"$paint$green  $no_paint The number of colleges that offer part-time education is: \"$num_univ\"              $paint$green  $no_paint\"\n",
    "echo \"$paint$green  $no_paint                                                                            $paint$green  $no_paint\"\n",
    "echo \"$paint$green                                                                                $no_paint\"\n",
    "\n",
    "#question 3\n",
    "echo \"$paint$yellow    QUESTION 3: PRINT THE PERCENTAGE OF COURSES IN ENGINEERING                  $no_paint\"\n",
    "echo \"$paint$yellow  $no_paint                                                                            $paint$yellow  $no_paint\"\n",
    "echo \"$paint$yellow  $no_paint The percentage of courses in engineering is: \"$z\"%                       $paint$yellow  $no_paint\"\n",
    "echo \"$paint$yellow  $no_paint                                                                            $paint$yellow  $no_paint\"\n",
    "echo \"$paint$yellow                                                                                $no_paint\"\n",
    "\n",
    "echo -e \"\\n\"\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "The screenshot below contains the output of the bash script runned on local PC command line using Ubuntu Linux:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![output_screenshot](CLQ_screen.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
