{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notebook by:\n",
    "* Lorenzo Pannacci 1948926\n",
    "* Francesco Proietti 1873188\n",
    "* Selin Topaloglu 2113300\n",
    "* Santiago Vessi 1958879"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Startup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################\n",
    "# LIBRARIES DOWNLOAD #\n",
    "######################\n",
    "\n",
    "install_packages = False\n",
    "if install_packages:\n",
    "    %pip install beautifulsoup4 tqdm pandas numpy matplotlib nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Utente\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Utente\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "####################\n",
    "# LIBRARIES IMPORT #\n",
    "####################\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from tqdm.notebook import tqdm\n",
    "import time\n",
    "import os\n",
    "import csv\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data collection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this homework, there is no provided dataset. Instead, you have to build your own. Your search engine will run on text documents. So, here\n",
    "we detail the procedure to follow for the data collection. We strongly suggest you work on different modules when implementing the required functions. For example, you may have a ```crawler.py``` module, a ```parser.py``` module, and a ```engine.py``` module: this is a good practice that improves readability in reporting and efficiency in deploying the code. Be careful; you are likely dealing with exceptions and other possible issues! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. Get the list of master's degree courses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start with the list of courses to include in your corpus of documents. In particular, we focus on web scrapping the [MSc Degrees](https://www.findamasters.com/masters-degrees/msc-degrees/). Next, we want you to **collect the URL** associated with each site in the list from the previously collected list.\n",
    "The list is long and split into many pages. Therefore, we ask you to retrieve only the URLs of the places listed in **the first 400 pages** (each page has 15 courses, so you will end up with 6000 unique master's degree URLs).\n",
    "\n",
    "The output of this step is a `.txt` file whose single line corresponds to the master's URL.\n",
    "\n",
    "---\n",
    "\n",
    "Firstly we observe that we can nagivate trough the different pages using the link `https://www.findamasters.com/masters-degrees/msc-degrees/?PG=n` and changing `n` with the number of the desidered page. We also observe that without waiting between a page load and the other after 30 pages the website thinks we are a bot and don't let us in, to fix this we decided wait some seconds before we open a new page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File already exist and is complete. Using the previous version.\n"
     ]
    }
   ],
   "source": [
    "data_folder_path = r\"data/\"\n",
    "if not os.path.exists(data_folder_path):                    # create main data folder if doesn't already exist\n",
    "    os.makedirs(data_folder_path)\n",
    "\n",
    "courses_urls_path = data_folder_path + r\"courses_urls.txt\"  # file path of the txt file to create\n",
    "sleep_time = 2                                              # idle time between to requests, to avoid being blocke\n",
    "to_crawl = False                                            # we check if the file already exists and has the right length, in this case we do not repeat the crawling\n",
    "n_pages = 400                                               # number of pages to search trough\n",
    "n_courses = n_pages * 15                                    # total number of courses crawled\n",
    "\n",
    "# to avoid the site block the crawler considering it a bot we use a user agent taken from a real chrome session\n",
    "headers = {\"user-agent\": r\"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36\"}\n",
    "\n",
    "if not os.path.exists(courses_urls_path): # if file does not exist we have to crawl\n",
    "    print(\"File does not exist! Crawling...\")\n",
    "    to_crawl = True\n",
    "\n",
    "if to_crawl == False: # if file is incomplete we have to crawl\n",
    "    with open(courses_urls_path, 'r') as file:\n",
    "        file_length = len(file.readlines())\n",
    "        if file_length < n_courses:\n",
    "            print(\"File exist but is incomplete! Crawling...\")\n",
    "            to_crawl = True\n",
    "        else:\n",
    "            print(\"File already exist and is complete. Using the previous version.\")\n",
    "\n",
    "\n",
    "# if data is missing go crawl\n",
    "if to_crawl == True:\n",
    "\n",
    "    with open(\"data/courses_urls.txt\", 'w') as file: # open file, if already exist creates a new one\n",
    "        for i in tqdm(range(n_pages)): # cycle trough every page\n",
    "            url = r\"https://www.findamasters.com/masters-degrees/msc-degrees/?PG=\" + str(1 + i) # we compose the url\n",
    "\n",
    "            # get the webpage\n",
    "            webpage = requests.get(url, headers = headers)\n",
    "            soup = BeautifulSoup(webpage.text)\n",
    "            soup.prettify()\n",
    "\n",
    "            tags = soup.find_all('a', {\"class\": \"courseLink\"})  # get the tags we are interested in\n",
    "\n",
    "            if not tags:\n",
    "                raise IOError(\"Crawler has been blocked by the website. Try again with higher idle time.\")\n",
    "\n",
    "            for tag in tags: # for every tag get the course link and append to file\n",
    "                link = tag[\"href\"]\n",
    "                file.write(r\"https://www.findamasters.com\" + link + \"\\n\")\n",
    "\n",
    "            time.sleep(sleep_time) # wait to avoid getting blocked\n",
    "\n",
    "    # the file automatically close itself when the \"with\" section ends, saving the written lines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Crawl master's degree pages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you get all the URLs in the first 400 pages of the list, you:\n",
    "\n",
    "1. Download the HTML corresponding to each of the collected URLs.\n",
    "2. After you collect a single page, immediately save its `HTML` in a file. In this way, if your program stops for any reason, you will not lose the data collected up to the stopping point.\n",
    "3. Organize the downloaded `HTML` pages into folders. Each folder will contain the `HTML` of the courses on page 1, page 2, ... of the list of master's programs.\n",
    "   \n",
    "__Tip__: Due to the large number of pages you should download, you can use some methods that can help you shorten the time. If you employed a particular process or approach, kindly describe it.\n",
    "\n",
    "---\n",
    "\n",
    "As before we have to insert a idle time between the loading of two pages to avoid that the website block us. We can found whether we have been blocked by checking if the webpage ha the title \"`Just a moment...`\". This makes the operations particularly slow, to crawl all pages we have to wait a few hours. To avoid to repeat this procedure more times than necessary we save the html pages and try to open them only if they are not already downloaded on the device."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All files already crawled. Using the existing version.\n"
     ]
    }
   ],
   "source": [
    "courses_pages_path = r\"data/courses_html_pages/\" # path of the folder containing all the subfolders with the html files\n",
    "\n",
    "# create folder if not exist already\n",
    "if not os.path.exists(courses_pages_path):\n",
    "    os.makedirs(courses_pages_path)\n",
    "\n",
    "# we check if the files already exists, in this case we do not repeat the crawling\n",
    "to_crawl = False\n",
    "files_count = 0\n",
    "for _, _, files in os.walk(courses_pages_path):\n",
    "    files_count += len(files)\n",
    "\n",
    "if files_count < n_courses:\n",
    "    print(\"Crawling...\")\n",
    "    to_crawl = True\n",
    "else:\n",
    "    print(\"All files already crawled. Using the existing version.\")\n",
    "\n",
    "# if data is missing go crawl\n",
    "if to_crawl == True:\n",
    "\n",
    "    # make a folder for every page if not already created\n",
    "    for i in range(1, 400 + 1):\n",
    "        folder_path = courses_pages_path + \"page_\" + str(i)\n",
    "        if not os.path.exists(folder_path):\n",
    "            os.makedirs(folder_path)\n",
    "\n",
    "    # populate folders\n",
    "    with open(courses_urls_path, 'r') as file_1:\n",
    "        for i, course_url in tqdm(enumerate(file_1), total = n_courses):\n",
    "            course_url = course_url.strip('\\n')\n",
    "            course_file_path = courses_pages_path + \"page_\" + str(1 + i // 15) + \"/\" + \"course_\" + str(1 + i % 15) + \".html\"\n",
    "\n",
    "            if not os.path.exists(course_file_path): # if already crawled do not repeat\n",
    "                # get page\n",
    "                webpage = requests.get(course_url, headers = headers)\n",
    "                soup = BeautifulSoup(webpage.text, \"html.parser\")\n",
    "\n",
    "                if soup.title.text == r\"Just a moment...\":\n",
    "                    raise IOError(\"Crawler has been blocked by the website. Try again with higher idle time.\")\n",
    "\n",
    "                # write file\n",
    "                with open(course_file_path, 'w+', encoding = \"utf-8\") as file_2:\n",
    "                    # html_page = soup.prettify()\n",
    "                    # file_2.write(str(html_page))\n",
    "                    file_2.write(str(soup))\n",
    "                # the file automatically close itself when the \"with\" section ends, saving the written lines\n",
    "\n",
    "                time.sleep(sleep_time) # wait to avoid getting blocked"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe that some pages as `https://www.findamasters.com/masters-degrees/course/emergency-management-and-resilience-msc/?i373d7361c25450` (page 215, course 3) are missing and gives us a filler webpage. We will have to treat those courses carefully as the only information we can get from those is the link. We can easily identify those kind of pages thanks to their title: \"`FindAMasters | 500 Error : Internal Server Error`\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# crawling correctness check\n",
    "\n",
    "print(\"Checking the correctness of the crawl operation...\")\n",
    "\n",
    "blocked_pages = 0\n",
    "unaviable_pages = 0\n",
    "correct_pages = 0\n",
    "for root, _, files in tqdm(os.walk(courses_pages_path), total = 401): # checks for files in 400 subfolders and on root folder, thus 401\n",
    "    for file in files:\n",
    "        course_file_path = os.path.join(root, file)\n",
    "\n",
    "        with open(course_file_path, 'r', encoding = \"utf-8\") as html_file:\n",
    "            html_content = html_file.read()\n",
    "\n",
    "        soup = BeautifulSoup(html_content, \"html.parser\")\n",
    "        page_title = soup.title.text\n",
    "\n",
    "        if page_title == r\"Just a moment...\": # blocked during crawling\n",
    "            blocked_pages += 1\n",
    "            os.remove(course_file_path)\n",
    "        elif page_title == r\"FindAMasters | 500 Error : Internal Server Error\": # missing on website\n",
    "            unaviable_pages += 1\n",
    "        else: # downloaded correctly\n",
    "            correct_pages += 1\n",
    "\n",
    "print(blocked_pages, \"pages were blocked during crawling and had been removed. If this value is not zero run the crawling again to get the missing pages.\")\n",
    "print(unaviable_pages, \"pages are not present on the website anymore.\")\n",
    "print(correct_pages, \"pages have been correctly downloaded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Parse downloaded pages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, you should have all the HTML documents about the master's degree of interest, and you can start to extract specific information. The list of the information we desire for each course and their format is as follows:\n",
    "\n",
    "1. Course Name (to save as ```courseName```): string;\n",
    "2. University (to save as ```universityName```): string;\n",
    "3. Faculty (to save as ```facultyName```): string\n",
    "4. Full or Part Time (to save as ```isItFullTime```): string;\n",
    "5. Short Description (to save as ```description```): string;\n",
    "6. Start Date (to save as ```startDate```): string;\n",
    "7. Fees (to save as ```fees```): string;\n",
    "8. Modality (to save as ```modality```):string;\n",
    "9. Duration (to save as ```duration```):string;\n",
    "10. City (to save as ```city```): string;\n",
    "11. Country (to save as ```country```): string;\n",
    "12. Presence or online modality (to save as ```administration```): string;\n",
    "13. Link to the page (to save as ```url```): string.\n",
    "\n",
    "<div style=\"overflow-x:auto;\">\n",
    "<table>\n",
    "<thead>\n",
    "  <tr>\n",
    "    <th>index</th>\n",
    "    <th>courseName</th>\n",
    "    <th>universityName</th>\n",
    "    <th>facultyName</th>\n",
    "    <th>isItFullTime</th>\n",
    "    <th>description</th>\n",
    "    <th>startDate</th>\n",
    "    <th>fees</th>\n",
    "    <th>modality</th>\n",
    "    <th>duration</th>\n",
    "    <th>city</th>\n",
    "    <th>country</th>\n",
    "    <th>administration</th>\n",
    "    <th>url</th>\n",
    "  </tr>\n",
    "</thead>\n",
    "<tbody>\n",
    "  <tr>\n",
    "    <td>0</td>\n",
    "    <td> Accounting and Finance - MSc</td>\n",
    "    <td>University of Leeds</td>\n",
    "    <td>Leeds University Business School</td>\n",
    "    <td>Full time</td>\n",
    "    <td>Businesses and governments rely on [...].</td>\n",
    "    <td>September</td>\n",
    "    <td>UK: £18,000 (Total) International: £34,750 (Total)</td>\n",
    "    <td>MSc</td>\n",
    "    <td>1 year full time</td>\n",
    "    <td>Leeds</td>\n",
    "    <td>United Kingdom</td>\n",
    "    <td>On Campus</td>\n",
    "    <td><a href=\"https://www.findamasters.com/masters-degrees/course/accounting-and-finance-msc/?i321d3232c3891\">Link</a></td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>1</td>\n",
    "    <td> Accounting, Accountability & Financial Management MSc</td>\n",
    "    <td>King’s College London</td>\n",
    "    <td>King’s Business School</td>\n",
    "    <td>Full time</td>\n",
    "    <td>Our Accounting, Accountability & Financial Management MSc course will provide [...].</td>\n",
    "    <td>September</td>\n",
    "    <td>Please see the university website for further information on fees for this course.</td>\n",
    "    <td>MSc</td>\n",
    "    <td>1 year FT</td>\n",
    "    <td>London</td>\n",
    "    <td>United Kingdom</td>\n",
    "    <td>On Campus</td>\n",
    "    <td><a href=\"https://www.findamasters.com/masters-degrees/course/accounting-accountability-and-financial-management-msc/?i132d7816c25522\">Link</a></td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>2</td>\n",
    "    <td> Accounting, Financial Management and Digital Business - MSc</td>\n",
    "    <td>University of Reading</td>\n",
    "    <td>Henley Business School</td>\n",
    "    <td>Full time</td>\n",
    "    <td>Embark on a professional accounting career [...].</td>\n",
    "    <td>September</td>\n",
    "    <td>Please see the university website for further information on fees for this course.</td>\n",
    "    <td>MSc</td>\n",
    "    <td>1 year full time</td>\n",
    "    <td>Reading</td>\n",
    "    <td>United Kingdom</td>\n",
    "    <td>On Campus</td>\n",
    "    <td><a href=\"https://www.findamasters.com/masters-degrees/course/accounting-financial-management-and-digital-business-msc/?i345d4286c351\">Link</a></td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>3</td>\n",
    "    <td> Addictions MSc</td>\n",
    "    <td>King’s College London</td>\n",
    "    <td>Institute of Psychiatry, Psychology and Neuroscience</td>\n",
    "    <td>Full time</td>\n",
    "    <td>Join us for an online session for prospective [...].</td>\n",
    "    <td>September</td>\n",
    "    <td>Please see the university website for further information on fees for this course.</td>\n",
    "    <td>MSc</td>\n",
    "    <td>One year FT</td>\n",
    "    <td>London</td>\n",
    "    <td>United Kingdom</td>\n",
    "    <td>On Campus</td>\n",
    "    <td><a href=\"https://www.findamasters.com/masters-degrees/course/addictions-msc/?i132d4318c27100\">Link</a></td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>4</td>\n",
    "    <td> Advanced Chemical Engineering - MSc</td>\n",
    "    <td>University of Leeds</td>\n",
    "    <td>School of Chemical and Process Engineering</td>\n",
    "    <td>Full time</td>\n",
    "    <td>The Advanced Chemical Engineering MSc at Leeds [...].</td>\n",
    "    <td>September</td>\n",
    "    <td>UK: £13,750 (Total) International: £31,000 (Total)</td>\n",
    "    <td>MSc</td>\n",
    "    <td>1 year full time</td>\n",
    "    <td>Leeds</td>\n",
    "    <td>United Kingdom</td>\n",
    "    <td>On Campus</td>\n",
    "  </tr>\n",
    "  <!-- Add more rows here as needed -->\n",
    "</tbody>\n",
    "</table>\n",
    "</div>\n",
    "\n",
    "\n",
    "For each master's degree, you create a `course_i.tsv` file of this structure:\n",
    "\n",
    "```\n",
    "courseName \\t universityName \\t  ... \\t url\n",
    "```\n",
    "\n",
    "If an information is missing, you just leave it as an empty string.\n",
    "\n",
    "---\n",
    "\n",
    "We can observe that some informations are \"mandatory\", that means that every page (that is not a filler page) has them while others could or could not be present. Meanwhile filler pages gives us no information whatsoever, for those we have only the page url. To create a `.tsv` file we can just use the Python `csv` module changing its delimiter with the character `\\t`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating .tsv files...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ff1cceb7de542fcab6b70f2beb091bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tsvs_path = r\"data/tsvs/\" # path of the folder containing all the .tsv files\n",
    "\n",
    "# create folder if not exist already\n",
    "if not os.path.exists(tsvs_path):\n",
    "    os.makedirs(tsvs_path)\n",
    "\n",
    "# we check if all the files already exists, in this case we do not repeat the crawling\n",
    "to_crawl = False\n",
    "files_count = 0\n",
    "for _, _, files in os.walk(tsvs_path):\n",
    "    files_count += len(files)\n",
    "\n",
    "if files_count < n_courses:\n",
    "    print(\"Creating .tsv files...\")\n",
    "    to_crawl = True\n",
    "else:\n",
    "    print(\"All files already created. Using the existing version.\")\n",
    "\n",
    "# if data is missing go crawl\n",
    "if to_crawl == True:\n",
    "    with open(courses_urls_path, 'r') as courses_file:\n",
    "        for i, url in tqdm(enumerate(courses_file), total = n_courses):\n",
    "            url = url.strip(\"\\n\")\n",
    "\n",
    "            # if file .tsv already exist skip its creation\n",
    "            tsv_file_path = tsvs_path + \"course_\" + str(1 + i) + \".tsv\"\n",
    "            if os.path.exists(tsv_file_path):\n",
    "                continue\n",
    "\n",
    "            # create path, open and read .html file\n",
    "            course_file_path = courses_pages_path + \"page_\" + str(1 + i // 15) + \"/\" + \"course_\" + str(1 + i % 15) + \".html\"\n",
    "            with open(course_file_path, 'r', encoding = \"utf-8\") as html_file:\n",
    "                html_content = html_file.read()\n",
    "\n",
    "            soup = BeautifulSoup(html_content, \"html.parser\")\n",
    "\n",
    "            # if the page is no avaiable we can't get informations\n",
    "            if soup.title.text == r\"FindAMasters | 500 Error : Internal Server Error\":\n",
    "                courseName = universityName = facultyName = isItFullTime = description = startDate = fees = modality = duration = city = country = administration = \"\"\n",
    "\n",
    "            else:\n",
    "                # get all the required fields\n",
    "\n",
    "                courseName = soup.find(\"h1\", {\"class\": \"course-header__course-title\"}).get_text(strip = True)\n",
    "                universityName = soup.find(\"a\", {\"class\": \"course-header__institution\"}).get_text(strip = True)\n",
    "                facultyName = soup.find(\"a\", {\"class\": \"course-header__department\"}).get_text(strip = True)\n",
    "\n",
    "                # some entries do not have this field\n",
    "                extract = soup.find(\"span\", {\"class\": \"key-info__study-type\"})\n",
    "                if extract is None:\n",
    "                    isItFullTime = \"\"\n",
    "                else:\n",
    "                    isItFullTime = extract.get_text(strip = True)\n",
    "\n",
    "                description = soup.find(\"div\", {\"class\": \"course-sections__description\"}).find(\"div\", {\"class\": \"course-sections__content\"}).get_text(strip = True, separator = \" \")\n",
    "                startDate = soup.find(\"span\", {\"class\": \"key-info__start-date\"}).get_text(strip = True)\n",
    "\n",
    "                # some entries do not have this field\n",
    "                extract = soup.find(\"div\", {\"class\": \"course-sections__fees\"})\n",
    "                if extract is None:\n",
    "                    fees = \"\"\n",
    "                else:\n",
    "                    fees = extract.find(\"div\", {\"class\": \"course-sections__content\"}).get_text(strip = True)\n",
    "\n",
    "                modality = soup.find(\"span\", {\"class\": \"key-info__qualification\"}).get_text(strip = True)\n",
    "                duration = soup.find(\"span\", {\"class\": \"key-info__duration\"}).get_text(strip = True)\n",
    "                city = soup.find(\"a\", {\"class\": \"course-data__city\"}).get_text(strip = True)\n",
    "                country = soup.find(\"a\", {\"class\": \"course-data__country\"}).get_text(strip = True)\n",
    "\n",
    "                # courses can be 'on_campus', 'online' or both, but this information is stored in different tags\n",
    "                extract1 = soup.find(\"a\", {\"class\": \"course-data__online\"})\n",
    "                extract2 = soup.find(\"a\", {\"class\": \"course-data__on-campus\"})\n",
    "                if extract1 is None and extract2 is None:\n",
    "                    administration = \"\"\n",
    "                elif extract2 is None:\n",
    "                    administration = extract1.get_text(strip = True)\n",
    "                elif extract1 is None:\n",
    "                    administration = extract2.get_text(strip = True)\n",
    "                else:\n",
    "                    administration = extract1.get_text(strip = True) + \" & \" + extract2.get_text(strip = True)\n",
    "\n",
    "            data = [[\"courseName\", \"universityName\", \"facultyName\", \"isItFullTime\", \"description\", \"startDate\", \"fees\", \"modality\", \"duration\", \"city\", \"country\", \"administration\", \"url\"],\n",
    "                    [courseName, universityName, facultyName, isItFullTime, description, startDate, fees, modality, duration, city, country, administration, url]]\n",
    "\n",
    "            with open(tsv_file_path, 'w+', newline='') as tsv_file:\n",
    "                writer = csv.writer(tsv_file, delimiter = '\\t', lineterminator = '\\n')\n",
    "                writer.writerows(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Search Engine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we want to create two different Search Engines that, given as input a query, return the courses that match the query."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.0 Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.0.0) Preprocessing the text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, you must pre-process all the information collected for each MSc by:\n",
    "\n",
    "1. Removing stopwords\n",
    "2. Removing punctuation\n",
    "3. Stemming\n",
    "4. Anything else you think it's needed\n",
    "   \n",
    "For this purpose, you can use the [`nltk library](https://www.nltk.org/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(txt):\n",
    "     \n",
    "    # convert to lowercase \n",
    "    # to treat the same words consistely\n",
    "    txt = txt.lower()\n",
    "    \n",
    "    # remove all punctuation\n",
    "    txt = re.sub(r'[^\\w\\s]', ' ', txt)\n",
    "\n",
    "    # tokenize the text\n",
    "    txt=txt.split()\n",
    "    \n",
    "    # remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    txt = [t for t in txt if t not in stop_words]\n",
    " \n",
    "    # stemming\n",
    "    stemmer = PorterStemmer()\n",
    "    txt = [stemmer.stem(t) for t in txt]\n",
    " \n",
    "    # Reassemble the text\n",
    "    txt = ' '.join(txt)\n",
    "\n",
    "    return txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.0.1) Preprocessing the fees column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Moreover, we want the field ```fees``` to collect numeric information. As you will see, you scraped textual information for this attribute in the dataset: sketch whatever method you need (using regex, for example, to find currency symbol) to collect information and, in case of multiple information, retrieve only the highest fees. Finally, once you have collected numerical information, you likely will have different currencies: this can be chaotic, so let chatGPT guide you in the choice and deployment of an API to convert this column to a common currency of your choice (it can be USD, EUR or whatever you want). Ultimately, you will have a ```float``` column renamed ```fees (CHOSEN COMMON CURRENCY)```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert currency TO EUR using Open Exchange Rates API\n",
    "def convert_to_eur(fees):\n",
    "    \n",
    "    conversion = 0.0\n",
    "    \n",
    "    # find currency symbol\n",
    "    currency = re.findall(r'[\\£\\$\\€]|GBP|USD|EUR|ISK|SEK|JPY|CHF', fees.upper())\n",
    "    #extract all numeric values, considering commas as thousand separator\n",
    "    matches = re.findall(r'(\\d+(?:,\\d{3})*(?:\\.\\d+)?)', fees)\n",
    "    \n",
    "    if currency and matches:\n",
    "        \n",
    "        # correct the numeric values for comparision\n",
    "        fees = [float(m.replace(',', '')) for m in matches]\n",
    "        # Find the highest fee\n",
    "        fee = max(fees)\n",
    "\n",
    "        # API request to get the latest exchange rates\n",
    "        base_url = \"https://open.er-api.com/v6/latest\"\n",
    "        API_KEY=\"6d56fb10262e4f29bef560d4c38fa3f4\"\n",
    "        params = {'base': currency, 'apiKey': API_KEY}\n",
    "        response = requests.get(base_url, params=params)\n",
    "        rates = response.json().get('rates', {})\n",
    "        # applies the conversion\n",
    "        conversion=np.round(fee / rates['EUR'], 2)\n",
    "\n",
    "    return conversion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. Conjunctive query"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the first version of the search engine, we narrowed our interest to the __description__ of each course. It means that you will evaluate queries only concerning the course's description."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.1) Create your index!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before building the index, \n",
    "* Create a file named `vocabulary`, in the format you prefer, that maps each word to an integer (`term_id`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start an empty dictionary\n",
    "vocabulary = {}\n",
    "# start with term_id as 1\n",
    "term_id = 1\n",
    "\n",
    "for i in range(1,6001):\n",
    "    tsv=\"course_\"+str(i)+\".tsv\"\n",
    "    file_path= os.path.join(tsvs_path,tsv)\n",
    "    with open(file_path, 'r', encoding='utf-8') as ff:\n",
    "        lines=ff.readlines()\n",
    "    \n",
    "    fields = lines[1].strip().split('\\t')\n",
    "    # Ensure that the list has enough elements\n",
    "    if len(fields) > 4:\n",
    "        \n",
    "        # preprocess the description\n",
    "        d=preprocess_text(fields[4])\n",
    "        # tokenize \n",
    "        words=d.split()\n",
    "        \n",
    "        for w in words:\n",
    "            if w not in vocabulary: #add to the vocabulary if the word is not in there yet\n",
    "                vocabulary[w]=term_id #assigns a unique id\n",
    "                term_id+=1 # update term_id\n",
    "\n",
    "# save the vocabulary to a file\n",
    "vocabulary_file_path = r\"data/vocabulary.txt\"\n",
    "with open(vocabulary_file_path, 'w', encoding='utf-8') as vocab:\n",
    "    for word, term_id in vocabulary.items():\n",
    "        vocab.write(f\"{word}\\t{term_id}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, the first brick of your homework is to create the Inverted Index. It will be a dictionary in this format:\n",
    "\n",
    "```\n",
    "{\n",
    "term_id_1:[document_1, document_2, document_4],\n",
    "term_id_2:[document_1, document_3, document_5, document_6],\n",
    "...}\n",
    "```\n",
    "where _document\\_i_ is the *id* of a document that contains that specific word.\n",
    "\n",
    "__Hint:__ Since you do not want to compute the inverted index every time you use the Search Engine, it is worth thinking about storing it in a separate file and loading it in memory when needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize an empty inverted_index\n",
    "inverted_index={}\n",
    "\n",
    "for i in range(1,6001):\n",
    "    tsv=\"course_\"+str(i)+\".tsv\"\n",
    "    file_path= os.path.join(tsvs_path,tsv)\n",
    "    with open(file_path, 'r', encoding='utf-8') as ff:\n",
    "        lines=ff.readlines()\n",
    "    \n",
    "    fields = lines[1].strip().split('\\t')\n",
    "    # Ensure that the list has enough elements\n",
    "    if len(fields) > 4:\n",
    "        # access the description field and tokenize the words,\n",
    "        d=preprocess_text(fields[4])\n",
    "        words=d.split()\n",
    "        for w in words:\n",
    "            # check if the word contains alphabetical characters or numbers\n",
    "            if w in vocabulary:\n",
    "                # update inverted index\n",
    "                id_term = vocabulary[w]\n",
    "                if id_term not in inverted_index:\n",
    "                    inverted_index[id_term]=[tsv]\n",
    "                else:\n",
    "                    if(tsv not in inverted_index[id_term]):\n",
    "                        inverted_index[id_term].append(tsv)\n",
    "\n",
    "\n",
    "# save the inverted index in a file\n",
    "inv_index_file_path = r\"data/inverted_index.txt\"\n",
    "with open(inv_index_file_path, 'w', encoding='utf-8') as inv_ind:\n",
    "    for word, term_id in inverted_index.items():\n",
    "        inv_ind.write(f\"{word}\\t{term_id}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.2) Execute the query\n",
    "Given a query input by the user, for example:\n",
    "\n",
    "```\n",
    "advanced knowledge\n",
    "```\n",
    "\n",
    "The Search Engine is supposed to return a list of documents.\n",
    "\n",
    "##### What documents do we want?\n",
    "Since we are dealing with conjunctive queries (AND), each returned document should contain all the words in the query.\n",
    "The final output of the query must return, if present, the following information for each of the selected documents:\n",
    "\n",
    "* `courseName`\n",
    "* `universityName`\n",
    "* `description`\n",
    "* `URL`\n",
    "\n",
    "If everything works well in this step, you can go to the next point and make your Search Engine more complex and better at answering queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_engine(query):\n",
    "    # print(preprocess_text(query))\n",
    "    query_words = preprocess_text(query).split()\n",
    "    # this list will contain all the docs that have the complete query in their description\n",
    "    doc = set() \n",
    "    for i in range(len(query_words)):\n",
    "        tmp=set() # we will need this to determine if a document contains all the query elements\n",
    "        w = query_words[i]\n",
    "        if w in vocabulary:\n",
    "            term_id = vocabulary[w]\n",
    "            if term_id in inverted_index:\n",
    "                t_id=inverted_index[term_id]\n",
    "                if i==0:\n",
    "                    doc.update(t_id)\n",
    "                else:\n",
    "                    tmp.update(t_id)\n",
    "       # filters out the documents that don't contain the previous word of the query\n",
    "        if i>0:\n",
    "            doc=doc.intersection(doc, tmp)\n",
    "                    \n",
    "\n",
    "    # Extract information from matching documents\n",
    "    result_data = []\n",
    "    for tsv_id in doc:\n",
    "        tsv_file = os.path.join(tsvs_path, f\"{tsv_id}\")\n",
    "        with open(tsv_file, 'r', encoding='utf-8') as ff:\n",
    "            lines=ff.readlines()\n",
    "        fields=lines[1].split(\"\\t\")\n",
    "        result_data.append({\n",
    "                    'courseName': fields[0],\n",
    "                    'universityName': fields[1],\n",
    "                    'description': fields[4],\n",
    "                    'URL': fields[-1],\n",
    "        })\n",
    "\n",
    "    # Create pandas DataFrame\n",
    "    result_df = pd.DataFrame(result_data)\n",
    "    return result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>courseName</th>\n",
       "      <th>universityName</th>\n",
       "      <th>description</th>\n",
       "      <th>URL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Master of Science in Individualized Genomics a...</td>\n",
       "      <td>Johns Hopkins University</td>\n",
       "      <td>Experience an Innovative Education in Genomics...</td>\n",
       "      <td>https://www.findamasters.com/masters-degrees/c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Chemistry - Environmental Analytical Chemistry...</td>\n",
       "      <td>University College Cork</td>\n",
       "      <td>MSc degree courses are provided in three key a...</td>\n",
       "      <td>https://www.findamasters.com/masters-degrees/c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Food Science (Food Biotechnology) - MSc</td>\n",
       "      <td>University of Leeds</td>\n",
       "      <td>Our Food Science (Food Biotechnology) MSc enga...</td>\n",
       "      <td>https://www.findamasters.com/masters-degrees/c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Applied Sport and Exercise Physiology - MSc/PG...</td>\n",
       "      <td>St Mary’s University, Twickenham</td>\n",
       "      <td>This degree involves a range of specialist mod...</td>\n",
       "      <td>https://www.findamasters.com/masters-degrees/c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Comparative Social Change - MSc</td>\n",
       "      <td>Trinity College Dublin</td>\n",
       "      <td>This course is offered jointly by the Departme...</td>\n",
       "      <td>https://www.findamasters.com/masters-degrees/c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>479</th>\n",
       "      <td>Civil Engineering - MSc</td>\n",
       "      <td>Abertay University</td>\n",
       "      <td>Our Civil Engineering MSc will give you a comp...</td>\n",
       "      <td>https://www.findamasters.com/masters-degrees/c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>480</th>\n",
       "      <td>Advanced Pharmacy Practice (PgCert/PgDip/MSc)</td>\n",
       "      <td>Robert Gordon University</td>\n",
       "      <td>The online MSc Advanced Pharmacy Practice cour...</td>\n",
       "      <td>https://www.findamasters.com/masters-degrees/c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>481</th>\n",
       "      <td>International Financial Analysis</td>\n",
       "      <td>Jonkoping University</td>\n",
       "      <td>Deepen your knowledge of financial markets and...</td>\n",
       "      <td>https://www.findamasters.com/masters-degrees/c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>482</th>\n",
       "      <td>Gerontological Nursing - MSc/PgDip</td>\n",
       "      <td>Trinity College Dublin</td>\n",
       "      <td>The aim of this course is to strengthen and de...</td>\n",
       "      <td>https://www.findamasters.com/masters-degrees/c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>483</th>\n",
       "      <td>Economics MSc</td>\n",
       "      <td>University of Nottingham</td>\n",
       "      <td>As the world becomes more complex and ever mor...</td>\n",
       "      <td>https://www.findamasters.com/masters-degrees/c...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>484 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            courseName  \\\n",
       "0    Master of Science in Individualized Genomics a...   \n",
       "1    Chemistry - Environmental Analytical Chemistry...   \n",
       "2              Food Science (Food Biotechnology) - MSc   \n",
       "3    Applied Sport and Exercise Physiology - MSc/PG...   \n",
       "4                      Comparative Social Change - MSc   \n",
       "..                                                 ...   \n",
       "479                            Civil Engineering - MSc   \n",
       "480      Advanced Pharmacy Practice (PgCert/PgDip/MSc)   \n",
       "481                   International Financial Analysis   \n",
       "482                 Gerontological Nursing - MSc/PgDip   \n",
       "483                                      Economics MSc   \n",
       "\n",
       "                       universityName  \\\n",
       "0            Johns Hopkins University   \n",
       "1             University College Cork   \n",
       "2                 University of Leeds   \n",
       "3    St Mary’s University, Twickenham   \n",
       "4              Trinity College Dublin   \n",
       "..                                ...   \n",
       "479                Abertay University   \n",
       "480          Robert Gordon University   \n",
       "481              Jonkoping University   \n",
       "482            Trinity College Dublin   \n",
       "483          University of Nottingham   \n",
       "\n",
       "                                           description  \\\n",
       "0    Experience an Innovative Education in Genomics...   \n",
       "1    MSc degree courses are provided in three key a...   \n",
       "2    Our Food Science (Food Biotechnology) MSc enga...   \n",
       "3    This degree involves a range of specialist mod...   \n",
       "4    This course is offered jointly by the Departme...   \n",
       "..                                                 ...   \n",
       "479  Our Civil Engineering MSc will give you a comp...   \n",
       "480  The online MSc Advanced Pharmacy Practice cour...   \n",
       "481  Deepen your knowledge of financial markets and...   \n",
       "482  The aim of this course is to strengthen and de...   \n",
       "483  As the world becomes more complex and ever mor...   \n",
       "\n",
       "                                                   URL  \n",
       "0    https://www.findamasters.com/masters-degrees/c...  \n",
       "1    https://www.findamasters.com/masters-degrees/c...  \n",
       "2    https://www.findamasters.com/masters-degrees/c...  \n",
       "3    https://www.findamasters.com/masters-degrees/c...  \n",
       "4    https://www.findamasters.com/masters-degrees/c...  \n",
       "..                                                 ...  \n",
       "479  https://www.findamasters.com/masters-degrees/c...  \n",
       "480  https://www.findamasters.com/masters-degrees/c...  \n",
       "481  https://www.findamasters.com/masters-degrees/c...  \n",
       "482  https://www.findamasters.com/masters-degrees/c...  \n",
       "483  https://www.findamasters.com/masters-degrees/c...  \n",
       "\n",
       "[484 rows x 4 columns]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q=\"advanced knowledge\"\n",
    "search_engine(q)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PART 2 to be continued..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Command Line Question"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As done in the previous assignment, we encourage using the command as a feature that Data Scientists must master.\n",
    "\n",
    "Note: To answer the question in this section, you must strictly use command line tools. We will reject any other method of response. The final script must be placed in CommandLine.sh.\n",
    "\n",
    "First, take the course_i.tsv files you created in point 1 and merge them using Linux commands (Hint: make sure that the first row containing the column names appears only once).\n",
    "\n",
    "Now that you have your merged file named merged_courses.tsv, use Linux commands to answer the following questions:\n",
    "- Which country offers the most Master's Degrees? Which city?\n",
    "- How many colleges offer Part-Time education?\n",
    "- Print the percentage of courses in Engineering (the word \"Engineer\" is contained in the course's name).\n",
    "\n",
    "__Important note:__ You may work on this question in any environment (AWS, your PC command line, Jupyter notebook, etc.), but the final script must be placed in CommandLine.sh, which must be executable. Please run the script and include a __screenshot__ of the <ins>output</ins> in the notebook for evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next cell contains the 'CommandLine.sh' script:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "\n",
    "#!/bin/bash\n",
    "\n",
    "\n",
    "#command useful in order to format the output\n",
    "paint=$(tput rev)\n",
    "no_paint=$(tput sgr 0)\n",
    "blue=$(tput setaf 4)\n",
    "red=$(tput setaf 1)\n",
    "green=$(tput setaf 2)\n",
    "yellow=$(tput setaf 3)\n",
    "\n",
    "#printing formatted title and introduction\n",
    "echo -e \"\\n\"\n",
    "echo \"$paint$red                      COMMAND LINE QUESTION HW3 AMDM                            $no_paint\"\n",
    "echo \"$paint$red  $no_paint                                                                            $paint$red  $no_paint\"\n",
    "echo \"$paint$red  $no_paint This bash script merges all the 6000 files .tsv in one and answers to      $paint$red  $no_paint\"\n",
    "echo \"$paint$red  $no_paint the three questions by analysing the .tsv file created.                    $paint$red  $no_paint\"\n",
    "echo \"$paint$red  $no_paint                                                                            $paint$red  $no_paint\"\n",
    "echo \"$paint$red                                                                                $no_paint\"\n",
    "echo -e \"\\n\"\n",
    "echo \"Please wait a few seconds, untill you see the result on standard output, the machine is calculating...\"\n",
    "echo \"For a clearly visualization of the output it's recommended to maximize the terminal window...\"\n",
    "echo -e \"\\n\"\n",
    "\n",
    "\n",
    "#################\n",
    "#               #\n",
    "# Merging files #---------------------------------------------------\n",
    "#               #\n",
    "#################\n",
    "\n",
    "#inizialization of the merged_file with the headers\n",
    "head -n1 course_1.tsv > merged_courses.tsv\n",
    "\n",
    "#appending rows to the merged_file\n",
    "for file in course*.tsv\n",
    "do\n",
    "    tail -n1 $file >> merged_courses.tsv\n",
    "done\n",
    "\n",
    "\n",
    "#################################\n",
    "#                               #\n",
    "# Which country and which city? #-----------------------------------\n",
    "#                               #\n",
    "#################################\n",
    "\n",
    "#assegnation of variable useful for calculate the max\n",
    "max_1=0\n",
    "max_country=' '\n",
    "\n",
    "#extracting the countries column \n",
    "cut -f11 merged_courses.tsv | sed 1d | sort -u > countries.tsv\n",
    "\n",
    "#this command says to the for loop to consider the entire line as a variable \n",
    "IFS=$'\\n'\n",
    "\n",
    "#for loop along all the countries\n",
    "for country in $(cat 'countries.tsv')\n",
    "do\n",
    "    #l contains the occurrence of the country\n",
    "    l=$(cut -f11 merged_courses.tsv | grep -i $country | wc -l)\n",
    "    \n",
    "    #if statement in order to compare and extract the max\n",
    "    if [ $l -ge $max_1 ]\n",
    "    then\n",
    "\tmax_1=$l\n",
    "\tmax_country=$country\n",
    "    fi\n",
    "done\n",
    "\n",
    "#this part works as the previous\n",
    "max_2=0\n",
    "max_city=' '\n",
    "cut -f10 merged_courses.tsv | sed 1d | sort -u > cities.tsv\n",
    "\n",
    "IFS=$'\\n'\n",
    "for city in $(cat 'cities.tsv')\n",
    "do\n",
    "    c=$(cut -f10 merged_courses.tsv | grep -i $city | wc -l)\n",
    "    if [ $c -ge $max_2 ]\n",
    "    then\n",
    "\tmax_2=$c\n",
    "\tmax_city=$city\n",
    "    fi\n",
    "done\n",
    "\n",
    "\n",
    "####################\n",
    "#                   #\n",
    "# Part-time courses #-----------------------------------------------\n",
    "#                   #\n",
    "#####################\n",
    "\n",
    "#extracting the columns of the university name and the time type\n",
    "cut -f2,4 merged_courses.tsv | sed 1d | grep -i 'part time' | cut -f1 > univ_p-t.tsv\n",
    "\n",
    "#sorting and deleting the duplicates we can calculate\n",
    "#the number of university that offers part-time courses\n",
    "num_univ=$(sort -u univ_p-t.tsv | wc -l)\n",
    "\n",
    "\n",
    "##########################################\n",
    "#                                        #\n",
    "# Calculating the courses in Engineering #--------------------------\n",
    "#                                        #\n",
    "##########################################\n",
    "\n",
    "#extracting the column about the courses' name\n",
    "cut -f1 merged_courses.tsv | sed 1d > courseName.tsv\n",
    "\n",
    "#counting the courses with 'Engineer' in their name\n",
    "x=$(grep -i \"Engineer\" courseName.tsv | wc -l)\n",
    "\n",
    "#counting the total courses (not counting the empty lines)\n",
    "y=$(grep -vc '^$' courseName.tsv)\n",
    "\n",
    "#calculating the percentage\n",
    "z=$(echo \"scale=3;$x*100.0/$y\" | bc)\n",
    "\n",
    "\n",
    "#removing the temporary files used for the analysis \n",
    "rm cities.tsv\n",
    "rm countries.tsv\n",
    "rm univ_p-t.tsv\n",
    "rm courseName.tsv\n",
    "\n",
    "#printing formatted output question 1\n",
    "echo \"$paint$blue    QUESTION 1: WHICH COUNTRY OFFERS THE MOST MASTER'S DEGREES? WHICH CITY?     $no_paint\"\n",
    "echo \"$paint$blue  $no_paint                                                                            $paint$blue  $no_paint\"\n",
    "echo \"$paint$blue  $no_paint The country that offers the greater number of Master's Degrees is:         $paint$blue  $no_paint\"\n",
    "echo \"$paint$blue  $no_paint \"$max_country\" with \"$max_1\" courses.                                          $paint$blue  $no_paint\"\n",
    "echo \"$paint$blue  $no_paint The city that offers the greater number of Master's Degree is: \"$max_city\"      $paint$blue  $no_paint\"\n",
    "echo \"$paint$blue  $no_paint with \"$max_2\" courses                                                          $paint$blue  $no_paint\"\n",
    "echo \"$paint$blue  $no_paint                                                                            $paint$blue  $no_paint\"\n",
    "echo \"$paint$blue                                                                                $no_paint\"\n",
    "\n",
    "#question 2\n",
    "echo \"$paint$green    QUESTION 2: HOW MANY COLLEGES OFFER PART-TIME EDUCATION?                    $no_paint\"\n",
    "echo \"$paint$green  $no_paint                                                                            $paint$green  $no_paint\"\n",
    "echo \"$paint$green  $no_paint The number of colleges that offer part-time education is: \"$num_univ\"              $paint$green  $no_paint\"\n",
    "echo \"$paint$green  $no_paint                                                                            $paint$green  $no_paint\"\n",
    "echo \"$paint$green                                                                                $no_paint\"\n",
    "\n",
    "#question 3\n",
    "echo \"$paint$yellow    QUESTION 3: PRINT THE PERCENTAGE OF COURSES IN ENGINEERING                  $no_paint\"\n",
    "echo \"$paint$yellow  $no_paint                                                                            $paint$yellow  $no_paint\"\n",
    "echo \"$paint$yellow  $no_paint The percentage of courses in engineering is: \"$z\"%                       $paint$yellow  $no_paint\"\n",
    "echo \"$paint$yellow  $no_paint                                                                            $paint$yellow  $no_paint\"\n",
    "echo \"$paint$yellow                                                                                $no_paint\"\n",
    "\n",
    "echo -e \"\\n\"\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "The screenshot below contains the output of the bash script runned on local PC command line using Ubuntu Linux:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![output_screenshot](CLQ_screen.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
