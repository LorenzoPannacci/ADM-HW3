{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notebook by:\n",
    "* Lorenzo Pannacci 1948926\n",
    "* INSERT NAME SURNAME AND ID HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Startup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################\n",
    "# LIBRARIES DOWNLOAD #\n",
    "######################\n",
    "\n",
    "install_packages = False\n",
    "if install_packages:\n",
    "    %pip install beautifulsoup4 tqdm pandas numpy matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################\n",
    "# LIBRARIES IMPORT #\n",
    "####################\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from tqdm.notebook import tqdm\n",
    "import time\n",
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Data collection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this homework, there is no provided dataset. Instead, you have to build your own. Your search engine will run on text documents. So, here\n",
    "we detail the procedure to follow for the data collection. We strongly suggest you work on different modules when implementing the required functions. For example, you may have a ```crawler.py``` module, a ```parser.py``` module, and a ```engine.py``` module: this is a good practice that improves readability in reporting and efficiency in deploying the code. Be careful; you are likely dealing with exceptions and other possible issues! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1. Get the list of master's degree courses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start with the list of courses to include in your corpus of documents. In particular, we focus on web scrapping the [MSc Degrees](https://www.findamasters.com/masters-degrees/msc-degrees/). Next, we want you to **collect the URL** associated with each site in the list from the previously collected list.\n",
    "The list is long and split into many pages. Therefore, we ask you to retrieve only the URLs of the places listed in **the first 400 pages** (each page has 15 courses, so you will end up with 6000 unique master's degree URLs).\n",
    "\n",
    "The output of this step is a `.txt` file whose single line corresponds to the master's URL.\n",
    "\n",
    "---\n",
    "\n",
    "Firstly we observe that we can nagivate trough the different pages using the link `https://www.findamasters.com/masters-degrees/msc-degrees/?PG=n` and changing `n` with the number of the desidered page. We also observe that after 30 pages are loaded the site thinks we are a bot and don't let us in, to fix this we wait some seconds before we open a new page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File already exist and is complete. Using the previous version.\n"
     ]
    }
   ],
   "source": [
    "data_folder_path = r\"data/\"\n",
    "if not os.path.exists(data_folder_path): # create main data folder if doesn't already exist\n",
    "    os.makedirs(data_folder_path)\n",
    "\n",
    "courses_urls_path = data_folder_path + r\"courses_urls.txt\" # file path of the txt file to create\n",
    "sleep_time = 2 # idle time between to requests, to avoid being blocke\n",
    "to_crawl = False # we check if the file already exists and has the right length, in this case we do not repeat the crawling\n",
    "n_pages = 400 # number of pages to search trough\n",
    "n_courses = n_pages * 15 # total number of courses crawled\n",
    "\n",
    "# to avoid the site block the crawler considering it a bot we use a user agent taken from a real chrome session\n",
    "headers = {\"user-agent\": r\"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36\"}\n",
    "\n",
    "if not os.path.exists(courses_urls_path): # if file does not exist we have to crawl\n",
    "    print(\"File does not exist! Crawling...\")\n",
    "    to_crawl = True\n",
    "\n",
    "if to_crawl == False: # if file is incomplete we have to crawl\n",
    "    with open(courses_urls_path, 'r') as file:\n",
    "        file_length = len(file.readlines())\n",
    "        if file_length < n_courses:\n",
    "            print(\"File exist but is incomplete! Crawling...\")\n",
    "            to_crawl = True\n",
    "        else:\n",
    "            print(\"File already exist and is complete. Using the previous version.\")\n",
    "\n",
    "\n",
    "# if data is missing go crawl\n",
    "if to_crawl == True:\n",
    "\n",
    "    links_crawled = 0\n",
    "    with open(\"data/courses_urls.txt\", 'w') as file: # open file, if already exist creates a new one\n",
    "        for i in tqdm(range(1, n_pages + 1)): # cycle trough every page\n",
    "            url = r\"https://www.findamasters.com/masters-degrees/msc-degrees/?PG=\" + str(i) # we compose the url\n",
    "\n",
    "            # get the webpage\n",
    "            webpage = requests.get(url, headers = headers)\n",
    "            soup = BeautifulSoup(webpage.text)\n",
    "            soup.prettify()\n",
    "\n",
    "            tags = soup.find_all('a', {\"class\": \"courseLink\"})  # get the tags we are interested in\n",
    "\n",
    "            if not tags:\n",
    "                raise IOError(\"Crawl has been blocked by the website. Try with higher idle time.\")\n",
    "\n",
    "            for tag in tags: # for every tag get the course link and append to file\n",
    "                link = tag[\"href\"]\n",
    "                file.write(r\"https://www.findamasters.com\" + link + \"\\n\")\n",
    "                links_crawled += 1\n",
    "\n",
    "            time.sleep(sleep_time) # wait to avoid getting blocked\n",
    "\n",
    "    # the file automatically close itself when the \"with\" section ends, saving the written lines\n",
    "\n",
    "    print(\"I crawled\", links_crawled, \"courses links!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2. Crawl master's degree pages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you get all the URLs in the first 400 pages of the list, you:\n",
    "\n",
    "1. Download the HTML corresponding to each of the collected URLs.\n",
    "2. After you collect a single page, immediately save its `HTML` in a file. In this way, if your program stops for any reason, you will not lose the data collected up to the stopping point.\n",
    "3. Organize the downloaded `HTML` pages into folders. Each folder will contain the `HTML` of the courses on page 1, page 2, ... of the list of master's programs.\n",
    "   \n",
    "__Tip__: Due to the large number of pages you should download, you can use some methods that can help you shorten the time. If you employed a particular process or approach, kindly describe it.\n",
    "\n",
    "---\n",
    "\n",
    "As before we have to insert a idle time between the loading of two pages to avoid that the website block us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Crawling...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c8ecb14bd52429f894503a00fa1cbb1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32md:\\File\\GitHub\\ADM-HW3\\main.ipynb Cell 12\u001b[0m line \u001b[0;36m4\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/File/GitHub/ADM-HW3/main.ipynb#X14sZmlsZQ%3D%3D?line=40'>41</a>\u001b[0m     file_2\u001b[39m.\u001b[39mwrite(\u001b[39mstr\u001b[39m(soup))\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/File/GitHub/ADM-HW3/main.ipynb#X14sZmlsZQ%3D%3D?line=41'>42</a>\u001b[0m \u001b[39m# the file automatically close itself when the \"with\" section ends\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/File/GitHub/ADM-HW3/main.ipynb#X14sZmlsZQ%3D%3D?line=43'>44</a>\u001b[0m time\u001b[39m.\u001b[39msleep(sleep_time)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "courses_pages_path = r\"data/courses_html_pages/\" # path of the folder containing all the subfolders with the html files\n",
    "\n",
    "# create folder if not exist already\n",
    "if not os.path.exists(courses_pages_path):\n",
    "    os.makedirs(courses_pages_path)\n",
    "\n",
    "# we check if the files already exists, in this case we do not repeat the crawling\n",
    "to_crawl = False\n",
    "files_count = 0\n",
    "for _, _, files in os.walk(courses_pages_path):\n",
    "    files_count += len(files)\n",
    "\n",
    "if files_count < n_courses:\n",
    "    print(\"Crawling...\")\n",
    "    to_crawl = True\n",
    "else:\n",
    "    print(\"All files already crawled. Using the previous version.\")\n",
    "\n",
    "# if data is missing go crawl\n",
    "if to_crawl == True:\n",
    "\n",
    "    # make a folder for every page if not already created\n",
    "    for i in range(1, 400 + 1):\n",
    "        folder_path = courses_pages_path + \"page_\" + str(i)\n",
    "        if not os.path.exists(folder_path):\n",
    "            os.makedirs(folder_path)\n",
    "\n",
    "    # populate folders\n",
    "    with open(courses_urls_path, 'r') as file_1:\n",
    "        for i, course_url in tqdm(enumerate(file_1), total = n_courses):\n",
    "            course_url = course_url.strip('\\n')\n",
    "            course_file_path = courses_pages_path + \"page_\" + str(1 + i // 15) + \"/\" + \"course_\" + str(1 + i % 15) + \".html\"\n",
    "\n",
    "            if not os.path.exists(course_file_path): # if already crawled do not repeat\n",
    "                # get page\n",
    "                webpage = requests.get(course_url, headers = headers)\n",
    "                soup = BeautifulSoup(webpage.text, \"html.parser\")\n",
    "\n",
    "                # write file\n",
    "                with open(course_file_path, 'w+', encoding = \"utf-8\") as file_2:\n",
    "                    file_2.write(str(soup))\n",
    "                # the file automatically close itself when the \"with\" section ends, saving the written lines\n",
    "\n",
    "                time.sleep(sleep_time) # wait to avoid getting blocked\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe that some pages as `https://www.findamasters.com/masters-degrees/course/emergency-management-and-resilience-msc/?i373d7361c25450` (page 215, course 3) are missing and gives us a filler webpage. We will have to treat those courses carefully as the only information we can get from those is the link."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "343107a0441e407f8f79f15e234fafe0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32md:\\File\\GitHub\\ADM-HW3\\main.ipynb Cell 14\u001b[0m line \u001b[0;36m1\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/File/GitHub/ADM-HW3/main.ipynb#X24sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39mfor\u001b[39;00m file \u001b[39min\u001b[39;00m files:\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/File/GitHub/ADM-HW3/main.ipynb#X24sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m     file_path \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(root, file)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/File/GitHub/ADM-HW3/main.ipynb#X24sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(file_path, \u001b[39m'\u001b[39m\u001b[39mr\u001b[39m\u001b[39m'\u001b[39m, encoding \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mutf-8\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mas\u001b[39;00m html_file:\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/File/GitHub/ADM-HW3/main.ipynb#X24sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m         html_content \u001b[39m=\u001b[39m html_file\u001b[39m.\u001b[39mread()\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/File/GitHub/ADM-HW3/main.ipynb#X24sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m     soup \u001b[39m=\u001b[39m BeautifulSoup(html_content, \u001b[39m\"\u001b[39m\u001b[39mhtml.parser\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32md:\\File\\Anaconda\\Lib\\site-packages\\IPython\\core\\interactiveshell.py:284\u001b[0m, in \u001b[0;36m_modified_open\u001b[1;34m(file, *args, **kwargs)\u001b[0m\n\u001b[0;32m    277\u001b[0m \u001b[39mif\u001b[39;00m file \u001b[39min\u001b[39;00m {\u001b[39m0\u001b[39m, \u001b[39m1\u001b[39m, \u001b[39m2\u001b[39m}:\n\u001b[0;32m    278\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    279\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mIPython won\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt let you open fd=\u001b[39m\u001b[39m{\u001b[39;00mfile\u001b[39m}\u001b[39;00m\u001b[39m by default \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    280\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    281\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39myou can use builtins\u001b[39m\u001b[39m'\u001b[39m\u001b[39m open.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    282\u001b[0m     )\n\u001b[1;32m--> 284\u001b[0m \u001b[39mreturn\u001b[39;00m io_open(file, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m<frozen codecs>:309\u001b[0m, in \u001b[0;36m__init__\u001b[1;34m(self, errors)\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# crawling quality check\n",
    "\n",
    "blocked_pages = 0\n",
    "unaviable_pages = 0\n",
    "correct_pages = 0\n",
    "for root, _, files in tqdm(os.walk(courses_pages_path), total = 6000):\n",
    "    for file in files:\n",
    "        file_path = os.path.join(root, file)\n",
    "\n",
    "        with open(file_path, 'r', encoding = \"utf-8\") as html_file:\n",
    "            html_content = html_file.read()\n",
    "\n",
    "        soup = BeautifulSoup(html_content, \"html.parser\")\n",
    "        page_title = soup.title.text\n",
    "\n",
    "        if page_title == r\"Just a moment...\": # blocked during crawling\n",
    "            blocked_pages += 1\n",
    "            os.remove(file_path)\n",
    "        elif page_title == r\"FindAMasters | 500 Error : Internal Server Error\":\n",
    "            unaviable_pages += 1\n",
    "        else:\n",
    "            correct_pages += 1\n",
    "\n",
    "print(blocked_pages, \"pages were blocked during crawling and had been removed. If there was any, run the crawling again to get them.\")\n",
    "print(unaviable_pages, \"pages are not present on the website anymore.\")\n",
    "print(correct_pages, \"pages have been correctly downloaded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Parse downloaded pages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, you should have all the HTML documents about the master's degree of interest, and you can start to extract specific information. The list of the information we desire for each course and their format is as follows:\n",
    "\n",
    "1. Course Name (to save as ```courseName```): string;\n",
    "2. University (to save as ```universityName```): string;\n",
    "3. Faculty (to save as ```facultyName```): string\n",
    "4. Full or Part Time (to save as ```isItFullTime```): string;\n",
    "5. Short Description (to save as ```description```): string;\n",
    "6. Start Date (to save as ```startDate```): string;\n",
    "7. Fees (to save as ```fees```): string;\n",
    "8. Modality (to save as ```modality```):string;\n",
    "9. Duration (to save as ```duration```):string;\n",
    "10. City (to save as ```city```): string;\n",
    "11. Country (to save as ```country```): string;\n",
    "12. Presence or online modality (to save as ```administration```): string;\n",
    "13. Link to the page (to save as ```url```): string.\n",
    "\n",
    "<div style=\"overflow-x:auto;\">\n",
    "<table>\n",
    "<thead>\n",
    "  <tr>\n",
    "    <th>index</th>\n",
    "    <th>courseName</th>\n",
    "    <th>universityName</th>\n",
    "    <th>facultyName</th>\n",
    "    <th>isItFullTime</th>\n",
    "    <th>description</th>\n",
    "    <th>startDate</th>\n",
    "    <th>fees</th>\n",
    "    <th>modality</th>\n",
    "    <th>duration</th>\n",
    "    <th>city</th>\n",
    "    <th>country</th>\n",
    "    <th>administration</th>\n",
    "    <th>url</th>\n",
    "  </tr>\n",
    "</thead>\n",
    "<tbody>\n",
    "  <tr>\n",
    "    <td>0</td>\n",
    "    <td> Accounting and Finance - MSc</td>\n",
    "    <td>University of Leeds</td>\n",
    "    <td>Leeds University Business School</td>\n",
    "    <td>Full time</td>\n",
    "    <td>Businesses and governments rely on [...].</td>\n",
    "    <td>September</td>\n",
    "    <td>UK: £18,000 (Total) International: £34,750 (Total)</td>\n",
    "    <td>MSc</td>\n",
    "    <td>1 year full time</td>\n",
    "    <td>Leeds</td>\n",
    "    <td>United Kingdom</td>\n",
    "    <td>On Campus</td>\n",
    "    <td><a href=\"https://www.findamasters.com/masters-degrees/course/accounting-and-finance-msc/?i321d3232c3891\">Link</a></td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>1</td>\n",
    "    <td> Accounting, Accountability & Financial Management MSc</td>\n",
    "    <td>King’s College London</td>\n",
    "    <td>King’s Business School</td>\n",
    "    <td>Full time</td>\n",
    "    <td>Our Accounting, Accountability & Financial Management MSc course will provide [...].</td>\n",
    "    <td>September</td>\n",
    "    <td>Please see the university website for further information on fees for this course.</td>\n",
    "    <td>MSc</td>\n",
    "    <td>1 year FT</td>\n",
    "    <td>London</td>\n",
    "    <td>United Kingdom</td>\n",
    "    <td>On Campus</td>\n",
    "    <td><a href=\"https://www.findamasters.com/masters-degrees/course/accounting-accountability-and-financial-management-msc/?i132d7816c25522\">Link</a></td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>2</td>\n",
    "    <td> Accounting, Financial Management and Digital Business - MSc</td>\n",
    "    <td>University of Reading</td>\n",
    "    <td>Henley Business School</td>\n",
    "    <td>Full time</td>\n",
    "    <td>Embark on a professional accounting career [...].</td>\n",
    "    <td>September</td>\n",
    "    <td>Please see the university website for further information on fees for this course.</td>\n",
    "    <td>MSc</td>\n",
    "    <td>1 year full time</td>\n",
    "    <td>Reading</td>\n",
    "    <td>United Kingdom</td>\n",
    "    <td>On Campus</td>\n",
    "    <td><a href=\"https://www.findamasters.com/masters-degrees/course/accounting-financial-management-and-digital-business-msc/?i345d4286c351\">Link</a></td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>3</td>\n",
    "    <td> Addictions MSc</td>\n",
    "    <td>King’s College London</td>\n",
    "    <td>Institute of Psychiatry, Psychology and Neuroscience</td>\n",
    "    <td>Full time</td>\n",
    "    <td>Join us for an online session for prospective [...].</td>\n",
    "    <td>September</td>\n",
    "    <td>Please see the university website for further information on fees for this course.</td>\n",
    "    <td>MSc</td>\n",
    "    <td>One year FT</td>\n",
    "    <td>London</td>\n",
    "    <td>United Kingdom</td>\n",
    "    <td>On Campus</td>\n",
    "    <td><a href=\"https://www.findamasters.com/masters-degrees/course/addictions-msc/?i132d4318c27100\">Link</a></td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>4</td>\n",
    "    <td> Advanced Chemical Engineering - MSc</td>\n",
    "    <td>University of Leeds</td>\n",
    "    <td>School of Chemical and Process Engineering</td>\n",
    "    <td>Full time</td>\n",
    "    <td>The Advanced Chemical Engineering MSc at Leeds [...].</td>\n",
    "    <td>September</td>\n",
    "    <td>UK: £13,750 (Total) International: £31,000 (Total)</td>\n",
    "    <td>MSc</td>\n",
    "    <td>1 year full time</td>\n",
    "    <td>Leeds</td>\n",
    "    <td>United Kingdom</td>\n",
    "    <td>On Campus</td>\n",
    "  </tr>\n",
    "  <!-- Add more rows here as needed -->\n",
    "</tbody>\n",
    "</table>\n",
    "</div>\n",
    "\n",
    "\n",
    "For each master's degree, you create a `course_i.tsv` file of this structure:\n",
    "\n",
    "```\n",
    "courseName \\t universityName \\t  ... \\t url\n",
    "```\n",
    "\n",
    "If an information is missing, you just leave it as an empty string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mUnable to start Kernel 'base (Python 3.11.4)' due to a timeout waiting for the ports to get used. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "tsvs_path = r\"data/tsvs/\" # path of the folder containing all the .tsv files\n",
    "\n",
    "# create folder if not exist already\n",
    "if not os.path.exists(tsvs_path):\n",
    "    os.makedirs(tsvs_path)\n",
    "\n",
    "# we check if the files already exists, in this case we do not repeat the crawling\n",
    "to_crawl = False\n",
    "files_count = 0\n",
    "for _, _, files in os.walk(tsvs_path):\n",
    "    files_count += len(files)\n",
    "\n",
    "if files_count < n_courses:\n",
    "    print(\"Crawling...\")\n",
    "    to_crawl = True\n",
    "else:\n",
    "    print(\"All files already crawled. Using the previous version.\")\n",
    "\n",
    "# if data is missing go crawl\n",
    "if to_crawl == True:\n",
    "    for i in tqdm(range(1, n_courses + 1)):\n",
    "        course_file_path = courses_pages_path + \"page_\" + str(1 + i // 15) + \"/\" + \"course_\" + str(1 + i % 15) + \".html\"\n",
    "\n",
    "        # open .html file with beautiful soup\n",
    "        # get informations needed\n",
    "        # create file and insert informations"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
